removeNumbers=T, # 숫자 제거
wordLengths=c(2, 5))) # 2~5자
inspect(tdm)
attributes(tdm)
tdm$dimnames
library(slam)
word.occur = as.array(rollup(tdm, 2))
length(word.occur)
word.order = order(word.occur, decreasing = T)
row.names(tdm)[word.order]
word.occur
most20 = word.order[1:20]
tdm.mat = as.matrix(tdm[most20,])
cormat = cor(t(tdm.mat))
font()
library(extrafont)
font()
qgraph(cormat, labels=rownames(cormat), diag=F, layout='spring'
, theme = 'Hollywood'
, label.scale=F
, label.font ='굴림')
library(qgraph)
qgraph(cormat, labels=rownames(cormat), diag=F, layout='spring'
, theme = 'Hollywood'
, label.scale=F
, label.font ='굴림')
qgraph(cormat, labels=rownames(cormat), diag=F, layout='spring'
, theme = 'Hollywood'
, label.scale=F
, label.font ='돋움')
qgraph(cormat, labels=rownames(cormat), diag=F, layout='spring'
, theme = 'Hollywood'
, label.scale=F
, label.font ='Gulim')
qgraph(cormat, labels=rownames(cormat), diag=F, layout='spring'
, theme = 'Hollywood'
, label.scale=F
)
qgraph(cormat, labels=rownames(cormat), diag=F, layout='spring'
, theme = 'Hollywood'
, label.scale=F
, labe.font='FangSong'
)
qgraph(cormat, labels=rownames(cormat), diag=F, layout='spring'
, theme = 'Hollywood'
, label.scale=F
, labe.font='Bitstream Vera Sans Mono'
)
qgraph(cormat, labels=rownames(cormat), diag=F, layout='spring'
, theme = 'Hollywood'
, label.scale=F
, label.font='Bitstream Vera Sans Mono'
)
font()
qgraph(cormat, labels=rownames(cormat), diag=F, layout='spring'
, theme = 'Hollywood'
, label.scale=F
, label.font = 'Nanumgothic'
)
qgraph(cormat, labels=rownames(cormat), diag=F, layout='spring'
, theme = 'Hollywood'
, label.scale=F
, label.font =1
)
qgraph(cormat, labels=rownames(cormat), diag=F, layout='spring'
, theme = 'Hollywood'
, label.scale=F
, label.font =2
)
qgraph(cormat, labels=rownames(cormat), diag=F, layout='spring'
, theme = 'Hollywood'
, label.scale=F
, label.font =3
)
qgraph(cormat, labels=rownames(cormat), diag=F, layout='spring'
, theme = 'Hollywood'
, label.scale=F
, label.font =4
)
qgraph(cormat, labels=rownames(cormat), diag=F, layout='spring'
, theme = 'Hollywood'
, label.scale=F
, label.font =4 , label.fill.vertical =1
)
qgraph(cormat, labels=rownames(cormat), diag=F, layout='spring'
, theme = 'Hollywood'
, label.scale=F
, label.font =4 , label.fill.vertical =0
)
qgraph(cormat, labels=rownames(cormat), diag=F, layout='spring'
, theme = 'Hollywood'
, label.scale=F
, label.font =4 , label.fill.horizontal =1
)
qgraph(cormat, labels=rownames(cormat), diag=F, layout='spring'
, theme = 'Hollywood'
, label.scale=F
, label.font =4 , label.fill.horizontal = 0
)
library(lsa)
tdm.mat = as.matrix(tdm)
# local weight
lw_tf(tdm.mat)
lw_bintf(tdm.mat)
lw_logtf(tdm.mat)
gw_normalisation(tdm.mat)
gw_idf(tdm.mat)
gw_gfidf(tdm.mat)
gw_entropy(tdm.mat)
library(tm)
library(lda)
library(topicmodels)
dtm = as.DocumentTermMatrix(tdm)
ldaform = dtm2ldaformat(dtm, omit_empty = T)
result.lda = lda.collapsed.gibbs.sampler(documents = ldaform$documents,
K = 10,
vocab = ldaform$vocab,
num.iterations = 5000,
burnin = 1000,
alpha = 0.01,
eta = 0.01)
attributes(result.lda)
result.lda$assignments
result.lda$topics
top.topic.words(result.lda$topics)[1:10,]
result.lda$topic_sums
result.lda$document_sums
alpha <- 0.02
eta <- 0.02
D <- length(ldaform$documents)  # number of documents (2,000)
W <- length(ldaform$vocab)  # number of terms in the vocab (14,568)
doc.length <- sapply(ldaform$documents, function(x) sum(x[2, ]))  # number of tokens per document [312, 288, 170, 436, 291, ...]
N <- sum(doc.length)  # total number of tokens in the data (546,827)
term.table <- table(unlist(word.order))
term.table <- sort(term.table, decreasing = TRUE)
term.frequency <- as.integer(term.table)  # frequencies of terms in the corpus [8939, 5544, 2411, 2410, 2143, ...]
theta <- t(apply(result.lda$document_sums + alpha, 2, function(x) x/sum(x)))
phi <- t(apply(t(result.lda$topics) + eta, 2, function(x) x/sum(x)))
CustComplaints <- list(phi = phi,
theta = theta,
doc.length = doc.length,
vocab = ldaform$vocab,
term.frequency = term.frequency)
library(LDAvis)
library(servr)
# create the JSON object to feed the visualization:
json <- createJSON(phi = CustComplaints$phi,
theta = CustComplaints$theta,
doc.length = CustComplaints$doc.length,
vocab = CustComplaints$vocab,
term.frequency = CustComplaints$term.frequency,
encoding="UTF-8",
family= "Nanumgothic")
serVis(json, out.dir = 'vis4', open.browser = FALSE, encoding = "UTF-8", family= "Nanumgothic")
setwd("C://R")
serVis(json, out.dir = 'vis4', open.browser = FALSE, encoding = "UTF-8", family= "Nanumgothic")
library(wordcloud)
m <- as.matrix(Tdm)
m <- as.matrix(tdm)
wordFreq <- sort(rowSums(m),decreasing=TRUE)
head(wordFreq)
wordFreq_F <- subset(wordFreq, wordFreq <=2)
wordFreq_F <- subset(wordFreq_F, wordFreq >=4)
length(wordFreq_F)
set.seed(length(wordFreq_F))
pal <- brewer.pal(8,"Dark2")
windowsFonts(malgun=windowsFont("맑은 고딕"))
wordcloud(words=names(wordFreq_F),freq=wordFreq_F
,min.freq=10 ,random.order=F ,rot.per=.1 ,colors=pal ,family="malgun")
wordcloud(words=names(wordFreq_F),freq=wordFreq_F
,min.freq=5 ,random.order=F ,rot.per=.1 ,colors=pal ,family="malgun")
wordFreq <- sort(rowSums(m),decreasing=TRUE)
wordFreq_F <- subset(wordFreq_F, wordFreq >=4)
length(wordFreq_F)
set.seed(length(wordFreq_F))
pal <- brewer.pal(8,"Dark2")
windowsFonts(malgun=windowsFont("맑은 고딕"))
wordcloud(words=names(wordFreq_F),freq=wordFreq_F
,min.freq=5 ,random.order=F ,rot.per=.1 ,colors=pal ,family="malgun")
wordFreq
wordFreq_F <- subset(wordFreq_F, wordFreq >1)
length(wordFreq_F)
set.seed(length(wordFreq_F))
pal <- brewer.pal(8,"Dark2")
windowsFonts(malgun=windowsFont("맑은 고딕"))
wordcloud(words=names(wordFreq_F),freq=wordFreq_F
,min.freq=5 ,random.order=F ,rot.per=.1 ,colors=pal ,family="malgun")
wordcloud(words=names(wordFreq_F),freq=wordFreq_F
,min.freq=1 ,random.order=F ,rot.per=.1 ,colors=pal ,family="malgun")
wordFreq
wordFreq_F
wordcloud(words=names(wordFreq),freq=wordFreq
,min.freq=1 ,random.order=F ,rot.per=.1 ,colors=pal ,family="malgun")
tdm <- TermDocumentMatrix(cps,
control=list(tokenize=ko.words, # 명사, 용언만 추출
removePunctuation=T, # 문장 부호 제거
removeNumbers=T, # 숫자 제거
wordLengths=c(2, 5)),
weighting = function(x) weightTfIdf(x, normalize = FALSE)
) # 2~5자
tdm <- TermDocumentMatrix(cps,
control=list(tokenize=ko.words, # 명사, 용언만 추출
removePunctuation=T, # 문장 부호 제거
removeNumbers=T, # 숫자 제거
wordLengths=c(2, 5))      # 2~5자
)
inspect(tdm)
library(wordcloud)
m <- as.matrix(tdm)
wordFreq <- sort(rowSums(m),decreasing=TRUE)
wordFreq
wordFreq_F <- subset(wordFreq_F, wordFreq > 1)
wordFreq_F
length(word.occur)
word.occur
word.order
most20
tdm.mat
word.occur
head(word.occur)
m
rowSums(m)
rowSums(m)
wordFreq <- sort(rowSums(m),decreasing=TRUE)
wordFreq
subset(wordFreq, wordFreq <=2)
wordFreq_F <- subset(wordFreq_F, wordFreq >=4)
wordFreq_F
wordFreq <- sort(rowSums(m),decreasing=TRUE)
subset(wordFreq_F, wordFreq >=4)
wordFreq_F <- subset(wordFreq, wordFreq >=4)
wordFreq_F
length(wordFreq_F)
set.seed(length(wordFreq_F))
pal <- brewer.pal(8,"Dark2")
windowsFonts(malgun=windowsFont("맑은 고딕"))
wordcloud(words=names(wordFreq_F),freq=wordFreq_F
,min.freq=1 ,random.order=F ,rot.per=.1 ,colors=pal ,family="malgun")
wordFreq
wordFreq[1:20]
wordFreq_F <- subset(wordFreq, wordFreq < 300)
wordFreq_F <- subset(wordFreq, wordFreq_F >=4)
length(wordFreq_F)
set.seed(length(wordFreq_F))
pal <- brewer.pal(8,"Dark2")
windowsFonts(malgun=windowsFont("맑은 고딕"))
wordcloud(words=names(wordFreq_F),freq=wordFreq_F
,min.freq=1 ,random.order=F ,rot.per=.1 ,colors=pal ,family="malgun")
wordFreq_F <- subset(wordFreq_F, wordFreq_F >=4)
length(wordFreq_F)
set.seed(length(wordFreq_F))
pal <- brewer.pal(8,"Dark2")
windowsFonts(malgun=windowsFont("맑은 고딕"))
wordcloud(words=names(wordFreq_F),freq=wordFreq_F
,min.freq=1 ,random.order=F ,rot.per=.1 ,colors=pal ,family="malgun")
wordcloud(words=names(wordFreq_F),freq=wordFreq_F
,min.freq=4 ,random.order=F ,rot.per=.1 ,colors=pal ,family="malgun")
length(wordFreq_F)
set.seed(length(wordFreq_F))
pal <- brewer.pal(8,"Dark2")
windowsFonts(malgun=windowsFont("맑은 고딕"))
wordcloud(words=names(wordFreq_F),freq=wordFreq_F
,min.freq=4 ,random.order=F ,rot.per=.1 ,colors=pal ,family="malgun")
wordFreq_F[1:20]
wordFreq_F <- subset(wordFreq, wordFreq < 300)
wordFreq_F[1:20]
wordFreq_F <- subset(wordFreq_F, wordFreq_F >=4)
set.seed(length(wordFreq_F))
pal <- brewer.pal(8,"Dark2")
windowsFonts(malgun=windowsFont("맑은 고딕"))
wordcloud(words=names(wordFreq_F),freq=wordFreq_F
,min.freq=4 ,random.order=F ,rot.per=.1 ,colors=pal ,family="malgun")
wordcloud(words=names(wordFreq_F),freq=wordFreq_F
,min.freq=1 ,random.order=F ,rot.per=.1 ,colors=pal ,family="malgun")
,min.freq=1 ,random.order=F ,rot.per=.05 ,colors=pal ,family="malgun")
wordcloud(words=names(wordFreq_F),freq=wordFreq_F
,min.freq=1 ,random.order=F ,rot.per=.05 ,colors=pal ,family="malgun")
set.seed(length(wordFreq_F))
pal <- brewer.pal(8,"Dark2")
windowsFonts(malgun=windowsFont("맑은 고딕"))
wordcloud(words=names(wordFreq_F),freq=wordFreq_F
,min.freq=1 ,random.order=F ,rot.per=.05 ,colors=pal ,family="malgun")
wordFreq_F <- subset(wordFreq_F, wordFreq_F >=10)
wordFreq_F[1:20]
set.seed(length(wordFreq_F))
pal <- brewer.pal(8,"Dark2")
windowsFonts(malgun=windowsFont("맑은 고딕"))
wordcloud(words=names(wordFreq_F),freq=wordFreq_F
,min.freq=1 ,random.order=F ,rot.per=.05 ,colors=pal ,family="malgun")
wordcloud(words=names(wordFreq_F),freq=wordFreq_F
,min.freq=1 ,random.order=F ,rot.per=.2 ,colors=pal ,family="malgun")
wordcloud(words=names(wordFreq_F),freq=wordFreq_F
,min.freq=1 ,random.order=T ,rot.per=1 ,colors=pal ,family="malgun")
wordcloud(words=names(wordFreq_F),freq=wordFreq_F
,min.freq=1 ,random.order=T ,rot.per=.5 ,colors=pal ,family="malgun")
wordcloud(words=names(wordFreq_F),freq=wordFreq_F
,min.freq=1 ,random.order=F ,rot.per=.5 ,colors=pal ,family="malgun")
library(httr)
library(rvest)
res = GET('http://cran.r-project.org/web/packages/httr/httr.pdf')
writeBin(content(res, 'raw'), 'httr.pdf')
h = html('http://gae9.com/trend/1DRlnSN7k1nb#!hot')
html_nodes(h, 'div.trend-post-content img')
html('http://gae9.com/trend/1DRlnSN7k1nb#!hot')
data(crude)
dtm <- DocumentTermMatrix(crude,
control = list(weighting = function(x) weightTfIdf(x, normalize = FALSE),
stopwords = TRUE))
dtm
inspect(dtm)
t(dtm)
tdm_c <- t(dtm)
tdm_c
t(tdm_c)
inspect(tdm_c)
crude
crude[1]
insepect(crude[1])
inspect(crude[1])
inspect(crude[1,1])
cps
dtm <- DocumentTermMatrix(cps,
control = list(weighting = function(x) weightTfIdf(x, normalize = FALSE),
stopwords = TRUE,
tokenize=ko.words, # 명사, 용언만 추출
removePunctuation=T, # 문장 부호 제거
removeNumbers=T, # 숫자 제거
wordLengths=c(2, 5)
))
dtm
tdm <- t(dtm)
inspect(tdm)
word.occur = as.array(rollup(tdm, 2))
length(word.occur)
word.occur
tdm_tf <- t(dtm)
m_tf <- as.matrix(tdm_tf)
wordFreq <- sort(rowSums(m_tf),decreasing=TRUE)
wordFreq[1:20]
wordFreq_F <- subset(wordFreq, wordFreq < 300)
wordFreq_F <- subset(wordFreq_F, wordFreq_F >=10)
wordFreq_F[1:20]
length(wordFreq_F)
set.seed(length(wordFreq_F))
pal <- brewer.pal(8,"Dark2")
windowsFonts(malgun=windowsFont("맑은 고딕"))
wordcloud(words=names(wordFreq_F),freq=wordFreq_F
,min.freq=1 ,random.order=F ,rot.per=.5 ,colors=pal ,family="malgun")
wordcloud(words=names(wordFreq_F),freq=wordFreq_F
,min.freq=2 ,random.order=F ,rot.per=.1 ,colors=pal ,family="malgun")
wordFreq_F <- subset(wordFreq_F, wordFreq_F >=30)
set.seed(50)
pal <- brewer.pal(8,"Dark2")
windowsFonts(malgun=windowsFont("맑은 고딕"))
wordcloud(words=names(wordFreq_F),freq=wordFreq_F
,min.freq=2 ,random.order=F ,rot.per=.1 ,colors=pal ,family="malgun")
set.seed(100)
pal <- brewer.pal(8,"Dark2")
windowsFonts(malgun=windowsFont("맑은 고딕"))
wordcloud(words=names(wordFreq_F),freq=wordFreq_F
,min.freq=2 ,random.order=F ,rot.per=.1 ,colors=pal ,family="malgun")
length(wordFreq_F)
wordFreq_F <- subset(wordFreq, wordFreq < 300)
wordFreq_F <- subset(wordFreq_F, wordFreq_F >=20)
length(wordFreq_F)
set.seed(80)
pal <- brewer.pal(8,"Dark2")
windowsFonts(malgun=windowsFont("맑은 고딕"))
wordcloud(words=names(wordFreq_F),freq=wordFreq_F
,min.freq=2 ,random.order=F ,rot.per=.1 ,colors=pal ,family="malgun")
paste(SimplePos09('하이브리드는 다 좋은데 갑자기 엔진이 돌 때 깜짝 놀랍니다'))
paste(SimplePos22('하이브리드는 다 좋은데 갑자기 엔진이 돌 때 깜짝 놀랍니다'))
extractNoun('하이브리드는 다 좋은데 갑자기 엔진이 돌 때 깜짝 놀랍니다')
gc()
extractNoun('하이브리드는 다 좋은데 갑자기 엔진이 돌 때 깜짝 놀랍니다')
a <- c('하이브리드는 다 좋은데 갑자기 엔진이 돌 때 깜짝 놀랍니다')
extractNoun(a)
useSejongDic()
extractNoun(a)
load("C:/R/Chinese/yoo/currentWS.RData")
library(coreNLP)
initCoreNLP(type='german')
downloadCoreNLP(type='german')
downloadCoreNLP()
initCoreNLP(type='german')
initCoreNLP(type='german')
doc = dat$Text[1]
dat = read.csv('data_german.csv', header=T, fileEncoding = 'utf8', stringsAsFactors = F)
Sys.setlocale(category = "LC_ALL", locale = "German")
setwd("C:\R\Chinese")
setwd("C://R//Chinese")
dat = read.csv('data_german.csv', header=T, fileEncoding = 'utf8', stringsAsFactors = F)
dat$Text[1]
doc = dat$Text[1]
output = annotateString(doc)
result = getToken(output)
result[grep('ADJA|NN|^V', result$POS), c('token', 'POS')]
View(result)
ger_tokenizer = function(doc){
doc <- as.character(doc)
output = annotateString(doc)
result = getToken(output)
result[grep('ADJA|NN|^V', result$POS),'token']
}
ger_tokenizer(dat$Text[3])
ger_tokenizer(dat$Text[2])
Sys.setlocale(category = "LC_ALL", locale = "German")
ger_tokenizer(dat$Text[2])
ger_tokenizer(dat$Text[3])
dat = read.csv('data_german.csv', header=T, fileEncoding = 'utf8', stringsAsFactors = F)
dat$Text[1]
output = annotateString(doc)
result = getToken(output)
result[grep('ADJA|NN|^V', result$POS), c('token', 'POS')]
ger_tokenizer(dat$Text[3])
library(tm)
cps <- Corpus(VectorSource(dat$Text))
tdm <- TermDocumentMatrix(cps,
control=list(tokenize=ger_tokenizer,
wordLengths=c(3,Inf)))
m1 <- as.matrix(tdm)
v <- sort(rowSums(m1), decreasing = TRUE)
d <- data.frame(word = names(v), freq = v)
d
tdm.mat = as.matrix(tdm)
sort(rowSums(tdm.mat), dec=T)[1:10]
library(dplyr)
new.data = cbind(dat, t(tdm.mat))
View(new.data)
aus.count = new.data %>%
group_by(Time) %>%
summarise(counting = sum(ausgeschlagen))
aus.count
library(ggplot2)
ggplot(aus.count, aes(Time, counting)) +
geom_bar(stat="identity") + coord_flip()
ggplot(aus.count, aes(dTime, counting)) + geom_point(aes(size = count), alpha = 1/2) + geom_smooth() + scale_size_area()
ggplot(aus.count, aes(Time, counting)) + geom_point(aes(size = count), alpha = 1/2) + geom_smooth() + scale_size_area()
View(aus.count)
ggplot(aus.count, aes(Time, counting)) + geom_point(aes(size = counting), alpha = 1/2) + geom_smooth() + scale_size_area()
vignette("introduction", package = "dplyr")
Sys.setlocale(category = "LC_ALL", locale = "English")
dat = read.csv('english.csv', header=T, fileEncoding = 'utf8', stringsAsFactors = F)
dat = read.csv('data_english.csv', header=T, fileEncoding = 'utf8', stringsAsFactors = F)
dat$Text[1]
dat = read.csv('data_english.csv', header=T, fileEncoding = 'utf8', stringsAsFactors = F)
dat$Text[1]
dat$Text[2]
dat$Text[3]
dat = read.csv('data_english.csv', header=T, fileEncoding = 'utf8', stringsAsFactors = F)
dat = read.csv('data_english.csv', header=T, fileEncoding = 'utf8', stringsAsFactors = T)
dat = read.csv('data_english1.csv', header=T, fileEncoding = 'utf8', stringsAsFactors = F)
dat$Text[3]
dat = read.csv('data_english1.csv', header=T, fileEncoding = 'utf8', stringsAsFactors = F)
dat$Text[3]
dat = read.csv('data_english2.csv', header=T, fileEncoding = 'utf8', stringsAsFactors = F)
dat = read.csv('english_data2.csv', header=T, fileEncoding = 'utf8', stringsAsFactors = F)
dat$Text[3]
dat = read.csv('english_data2.csv', header=T, fileEncoding = 'utf8', stringsAsFactors = F)
dat = read.csv('english_data2.csv', header=T, fileEncoding = 'utf8', stringsAsFactors = F)
dat = read.csv('english_data2.csv', header=F, fileEncoding = 'utf8', stringsAsFactors = F)
dat = read.csv('english_data2.csv', header=F, fileEncoding = 'utf8', stringsAsFactors = F)
dat = read.csv('english_data2.csv', header=F, fileEncoding = 'utf8', stringsAsFactors = F)
dat = read.csv('english_data2.csv', header=F, fileEncoding = 'utf8', stringsAsFactors = F)
dat = read.csv('english_data2.csv', header=F, fileEncoding = 'utf8')
dat$Text[3]
dat$Text[1]
dat = read.csv('test1.csv', header=F, fileEncoding = 'utf8')
dat = read.csv('test1.csv', sep = ",", quote = '"', header=F, fileEncoding = 'utf8')
dat = read.csv(file = 'test1.csv', sep = ",", quote = '"', header=F, fileEncoding = 'utf8')
dat = read.csv(file = 'data_english2.csv', sep = ",", quote = '"', header=F, fileEncoding = 'utf8')
dat$Text[1]
dat = read.csv(file = 'data_english2.csv', sep = ",", quote = '"', header=F, fileEncoding = 'utf8')
dat = read.csv(file = 'data_english1.csv', sep = ",", quote = '"', header=F, fileEncoding = 'utf8')
dat = read.csv(file = 'data_english.csv', sep = ",", quote = '"', header=F, fileEncoding = 'utf8')
dat = read.table(file = 'data_english.csv', sep = ",", quote = '"', header=F, fileEncoding = 'utf8')
dat = read.table(file = 'data_english.csv', sep = ",", quote = '"', header=F, fileEncoding = 'UTF-8')
dat = read.table(file = 'data_english2.csv', sep = ",", quote = '"', header=F, fileEncoding = 'UTF-8')
dat = read.table(file = 'data_english2.csv', sep = ",", quote = '"', header=T, fileEncoding = 'UTF-8')
dat = read.csv(file = 'data_english2.csv', sep = ",", quote = '"', header=T, fileEncoding = 'UTF-8')
dat = read.csv(file = 'data_english.csv', sep = ",", quote = '"', header=T, fileEncoding = 'UTF-8')
dat = read.csv(file = 'test1.csv', sep = ",", quote = '"', header=T, fileEncoding = 'UTF-8')
dat = read.csv(file = 'test1.csv', sep = ",", quote = '"', header=T, fileEncoding = 'UTF8')
dat = read.csv2(file = 'test1.csv', sep = ",", quote = '"', header=T, fileEncoding = 'UTF8')
dat = read.csv2(file = 'test1.csv', sep = ",", quote = '"', header=T, fileEncoding = 'UTF8')
dat
dat = read.csv2(file = '1.txt', sep = ",", quote = '"', header=T, fileEncoding = 'UTF8')
dat = read.csv2(file = '1.txt', sep = ",", quote = '"', header=T, fileEncoding = 'UTF8')
dat$Text[1]
dat
dat = read.csv2(file = '1.txt', sep = ",", quote = '"', header=T, fileEncoding = 'UTF8')
dat
View(dat)
dat = read.csv2(file = '1.txt', sep = ",", quote = '"', header=T, fileEncoding = 'UTF8')
dat
dat = read.csv2(file = '1.txt', sep = ",", quote = '"', header=T, fileEncoding = 'UTF8')
dat
View(dat)
dat = read.csv2(file = '1.txt', sep = ",", quote = '"', header=T, fileEncoding = 'utf8')
dat = read.csv2(file = '1.txt', sep = ",", quote = '"', header=T, fileEncoding = 'utf-8')
dat = read.csv2(file = '1.txt', sep = ",", quote = '"', header=T, encoding = 'utf-8')
dat
View(dat)
dat = read.csv2(file = '1.txt', sep = ",", quote = '"', header=T, encoding = 'utf-8')
dat
dat = read.csv2(file = '1.txt', sep = ",", quote = '"', header=T, encoding = 'utf-8')
dat
original.locale = Sys.getlocale('LC_CTYPE')
dat = read.csv2(file = 'data_english1.csv', sep = ",", quote = '"', header=T, encoding = 'utf-8')
dat
dat = read.csv2(file = 'data_english.csv', sep = ",", quote = '"', header=T, encoding = 'utf-8')
dat
