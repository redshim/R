options=list(
sankey="{link: {color: { fill: '#d799ae' } },
node: { width: 4,
color: { fill: '#a61d4c' },
label: { fontName: 'Times-Roman',
fontSize: 14,
color: '#871b47',
bold: true,
italic: true } }}"))
)
df
df = df[1:10,]
plot(
gvisSankey(df, from="source",
to="target", weight="value",
options=list(
sankey="{link: {color: { fill: '#d799ae' } },
node: { width: 4,
color: { fill: '#a61d4c' },
label: { fontName: 'Times-Roman',
fontSize: 14,
color: '#871b47',
bold: true,
italic: true } }}"))
)
plot(
gvisSankey(df, from="source",
to="target", weight="value",
options=list(
sankey="{link: {color: { fill: '#d799ae' } },
node: { width: 4,
color: { fill: '#a61d4c' },
label: { fontName: 'Times-Roman',
fontSize: 14,
color: '#871b47',
bold: true,
italic: true } }}"))
)
plot(
gvisSankey(df, from="source",
to="target", weight="value",
options=list(
sankey="{link: {color: { fill: '#d799ae' } },
node: { width: 4,
color: { fill: '#a61d4c' },
label: { fontName: 'Times-Roman',
fontSize: 14,
color: '#871b47',
bold: true,
italic: true } }}"))
)
require(igraph)
require(googleVis)
g <- graph.tree(24, children = 4)
set.seed(123)
E(g)$weight = rpois(23, 4) + 1
edgelist <- get.data.frame(g)
colnames(edgelist) <- c("source","target","value")
edgelist$source <- LETTERS[edgelist$source]
edgelist$target <- LETTERS[edgelist$target]
plot(
gvisSankey(edgelist, from="source",
to="target", weight="value",
options=list(
sankey="{link: {color: { fill: '#d799ae' } },
node: { width: 4,
color: { fill: '#a61d4c' },
label: { fontName: 'Times-Roman',
fontSize: 14,
color: '#871b47',
bold: true,
italic: true } }}"))
)
edgelist
g
E(g)$weight
g <- graph.tree(30, children = 4)
set.seed(123)
E(g)$weight = rpois(23, 4) + 1
g
get.data.frame(g)
edgelist <- get.data.frame(g)
colnames(edgelist) <- c("source","target","value")
edgelist$source <- LETTERS[edgelist$source]
edgelist$target <- LETTERS[edgelist$target]
edgelist
plot(
gvisSankey(edgelist, from="source",
to="target", weight="value",
options=list(
sankey="{link: {color: { fill: '#d799ae' } },
node: { width: 4,
color: { fill: '#a61d4c' },
label: { fontName: 'Times-Roman',
fontSize: 14,
color: '#871b47',
bold: true,
italic: true } }}"))
)
g <- graph.tree(100, children = 4)
set.seed(123)
E(g)$weight = rpois(23, 4) + 1
edgelist <- get.data.frame(g)
colnames(edgelist) <- c("source","target","value")
edgelist$source <- LETTERS[edgelist$source]
edgelist$target <- LETTERS[edgelist$target]
edgelist
plot(
gvisSankey(edgelist, from="source",
to="target", weight="value",
options=list(
sankey="{link: {color: { fill: '#d799ae' } },
node: { width: 4,
color: { fill: '#a61d4c' },
label: { fontName: 'Times-Roman',
fontSize: 14,
color: '#871b47',
bold: true,
italic: true } }}"))
)
d = read.csv2('C://Gephi//ext25.csv'
, header=F
, sep = ','
, quote = '"'
, stringsAsFactors = F)
UKvisits <- data.frame(origin=c(
"France", "Germany", "USA",
"Irish Republic", "Netherlands",
"Spain", "Italy", "Poland",
"Belgium", "Australia",
"Other countries", rep("UK", 5)),
visit=c(
rep("UK", 11), "Scotland",
"Wales", "Northern Ireland",
"England", "London"),
weights=c(
c(12,10,9,8,6,6,5,4,4,3,33)/100*31.8,
c(2.2,0.9,0.4,12.8,15.5)))
UKvisits
plot(
gvisSankey(df2, from="source",
to="target", weight="value",
options=list(
height=1000,
sankey="{link:{color:{fill:'lightblue'}}}"
))
)
require(googleVis)
plot(
gvisSankey(df, from="source",
to="target", weight="value",
options=list(
height=1000,
sankey="{link:{color:{fill:'lightblue'}}}"
))
)
df
df[1:5,]
df <- df[1:5,]
plot(
gvisSankey(df, from="source",
to="target", weight="value",
options=list(
height=1000,
sankey="{link:{color:{fill:'lightblue'}}}"
))
)
df <- df[1:4,]
plot(
gvisSankey(df, from="source",
to="target", weight="value",
options=list(
height=1000,
sankey="{link:{color:{fill:'lightblue'}}}"
))
)
plot(
gvisSankey(df, from="source",
to="target", weight="value",
options=list(
height=300,
sankey="{link:{color:{fill:'lightblue'}}}"
))
)
# load termDocMatrix
load("C:/R/igraph/termDocMatrix.rdata")
# inspect part of the matrix
termDocMatrix[5:10,1:20]
termDocMatrix[0:10,1:30]
as.matrix(termDocMatrix)
termDocMatrix <- as.matrix(termDocMatrix)
# change it to a Boolean matrix
termDocMatrix[termDocMatrix>=1] <- 1
termDocMatrix %*% t(termDocMatrix)
# transform into a term-term adjacency matrix
termMatrix <- termDocMatrix %*% t(termDocMatrix)
# inspect terms numbered 5 to 10
termMatrix[5:10,5:10]
library(igraph)
# build a graph from the above matrix
g <- graph.adjacency(termMatrix, weighted=T, mode = "undirected")
# remove loops
g <- simplify(g)
# set labels and degrees of vertices
V(g)$label <- V(g)$name
V(g)$degree <- degree(g)
######################################
# MBuild a Graph
######################################
# set seed to make the layout reproducible
set.seed(3952)
layout1 <- layout.fruchterman.reingold(g)
plot(g, layout=layout1)
######################################
# plot a Graph
######################################
plot(g, layout=layout.kamada.kawai)
tkplot(g, layout=layout.kamada.kawai)
######################################
# Make it Look Better
######################################
V(g)$label.cex <- 2.2 * V(g)$degree / max(V(g)$degree)+ .2
V(g)$label.color <- rgb(0, 0, .2, .8)
V(g)$frame.color <- NA
egam <- (log(E(g)$weight)+.4) / max(log(E(g)$weight)+.4)
E(g)$color <- rgb(.5, .5, 0, egam)
E(g)$width <- egam
# plot the graph in layout1
plot(g, layout=layout1)
termDocMatrix
termMatrix[5:10,5:10]
# build a graph from the above matrix
g <- graph.adjacency(termMatrix, weighted=T, mode = "undirected")
# remove loops
g <- simplify(g)
# set labels and degrees of vertices
V(g)$label <- V(g)$name
V(g)$degree <- degree(g)
# build a graph from the above matrix
g <- graph.adjacency(termMatrix, weighted=T, mode = "undirected")
# remove loops
g <- simplify(g)
# set labels and degrees of vertices
V(g)$label <- V(g)$name
V(g)$degree <- degree(g)
######################################
# MBuild a Graph
######################################
# set seed to make the layout reproducible
set.seed(3952)
layout1 <- layout.fruchterman.reingold(g)
plot(g, layout=layout1)
# plot a Graph
######################################
plot(g, layout=layout.kamada.kawai)
tkplot(g, layout=layout.kamada.kawai)
plot(g, layout=layout.kamada.kawai)
V(g)$label.cex
V(g)$label.cex <- 2.2 * V(g)$degree / max(V(g)$degree)+ .2
V(g)$label.color <- rgb(0, 0, .2, .8)
V(g)$frame.color <- NA
egam <- (log(E(g)$weight)+.4) / max(log(E(g)$weight)+.4)
E(g)$color <- rgb(.5, .5, 0, egam)
E(g)$width <- egam
# plot the graph in layout1
plot(g, layout=layout1)
setwd("C://R//TextAnalysis//English//Tesla")
rm(list=ls())
gc() # Garbage Collection = Clear memory Usage
options(mc.cores=1) # Use Core 1
getwd()
library(data.table)
library(tm)
library(wordcloud2)
library(ggplot2)
library(qgraph)
library(SnowballC)
library(readxl)
library(tokenizers)
d <- read_excel("tesla.xlsx")
writeLines(as.character(d$CONTENTS[[1]]))
df <- as.data.frame(d)
removeDoc <- function(x) {gsub("@[[:graph:]]*", "", x)}
removeURL <- function(x) { gsub("http://[[:graph:]]*", "", x)}
trim <- function (x) {gsub("^\\s+|\\s+$", "", x)}
removenum <- function (x) {gsub("\\d+", "", x)}
df$ptext <- tolower(df$CONTENTS)
df$ptext <- sapply(df$ptext, removeURL)
df$ptext <- sapply(df$ptext, removeDoc)
df$ptext <- sapply(df$ptext, trim)
df$ptext <- sapply(df$ptext, removenum)
df$ptext <- sapply(df$ptext, function(x) {gsub("if", "", x)})
df$ptext[1000]
df$ptext[1]
myStopwords <- c(stopwords('en'),"rt","the","it","is","this","would","the",
"i","one","of","or","be","and","did","now","going","has","could")
tokenize_words(df$ptext[1], stopwords = myStopwords)
en_tokenizer = function(x){
x <- as.character(x)
x_token = tokenize_words(x, stopwords = myStopwords)
return(x_token)
}
en_tokenizer(df$ptext[1])
library(tm)
cps <- Corpus(VectorSource(df$ptext))
cps <- tm_map(cps, removePunctuation)
cps <- tm_map(cps, removeNumbers)
#cps <- tm_map(cps, stemDocument)
cps <- tm_map(cps, stripWhitespace)
#cps <- tm_map(cps, stemDocument, language = "english")
toSpace <- content_transformer(function(x, pattern) { return (gsub(pattern, " ", x))})
cps <- tm_map(cps, toSpace, "-")
tdm <- TermDocumentMatrix(cps,
control=list(tokenize = en_tokenizer
, wordLengths=c(2,8))) # extract over 2 word
inspect(tdm)
tdm_term <- as.matrix(tdm$dimnames$Terms)
tdm_term
tdm <- removeSparseTerms(tdm, 0.995)
tdm$dimnames$Terms
tdm <- TermDocumentMatrix(cps,
control=list(tokenize = en_tokenizer
, wordLengths=c(2,8))) # extract over 2 word
tdm$dimnames$Terms
tdm
tdm <- TermDocumentMatrix(cps,
control=list(tokenize = en_tokenizer
, wordLengths=c(2,30))) # extract over 2 word
tdm
tdm$dimnames$Terms
en_tokenizer(df$ptext[1])
en_tokenizer(df$ptext[1:2])
chs_tokenizer = function(doc){
doc <- as.character(doc)
a = segmentCN(doc, nature = T, nosymbol = T)
dat = matrix(cbind(a, names(a)), ncol=2)
return(dat[grep('^[vna]+', dat[,2]),1])
}
setwd("C://R//TextAnalysis//Chinese - Training")
setwd("C:/R")
d <- read_excel("sail_review_100.xlsx",skip = 1)
setwd("C://R//TextAnalysis//Chinese - Training")
writeLines(as.character(d$contents[[1]]))
d <- read_excel("sail_review_100.xlsx",skip = 1)
writeLines(as.character(d$contents[[1]]))
library(Rwordseg)
a <- d$contents
tmplists = c()
for(split_doc in a){
splitlists = str_split(split_doc, '\\【', n=Inf)
for(alist in splitlists){
tmplists = c(tmplists,alist)
}
}
tmplists
tmplists[1]
a <- d$contents
a
tmplists = c()
for(split_doc in a){
splitlists = str_split(split_doc, '\\【', n=Inf)
for(alist in splitlists){
tmplists = c(tmplists,alist)
}
}
library(stringr)
for(split_doc in a){
splitlists = str_split(split_doc, '\\【', n=Inf)
for(alist in splitlists){
tmplists = c(tmplists,alist)
}
}
tmplists
tmplists[1]
a_conv1 <- subset(tmplists, (tmplists != "" & grepl("动力】", tmplists)))
a = segmentCN(a_conv[3], nature = T, nosymbol = T) # word separation
a_conv <- subset(tmplists, (tmplists != "" & grepl("内饰】", tmplists) & grepl("动力】", tmplists)))
a_conv2 <- subset(tmplists, (tmplists != "" & grepl("内饰】", tmplists)))
a_conv <- subset(tmplists, (tmplists != "" & grepl("内饰】", tmplists) & grepl("动力】", tmplists)))
a_conv <- c(a_conv1,a_conv2)
a = segmentCN(a_conv[3], nature = T, nosymbol = T) # word separation
a
chs_tokenizer = function(doc){
doc <- as.character(doc)
a = segmentCN(doc, nature = T, nosymbol = T)
dat = matrix(cbind(a, names(a)), ncol=2)
return(dat[grep('^[vna]+', dat[,2]),1])
}
chs_tokenizer(a_conv[2])
en_tokenizer(df$ptext[1:2])
x <- as.character(df$ptext[1])
x_token = tokenize_words(x, stopwords = myStopwords)
x_token
x_token[1]
x_token[,1]
matrix(x_token)
x_token = tokenize_words(x, stopwords = myStopwords)
x_token
class(x_token)
length(x_token)
nrow(x_token)
as.data.frame(x_token)
x_token = as.data.frame(x_token)
row.names(x_token) <- "token"
row.names(x_token) <- c("seq","token")
x_token = as.data.frame(x_token)
x_token
View(x_token)
row.names(x_token) <- c(token")
##################################################################################
#
# Create Corpus Object
#
##################################################################################
library(tm)
# 1.keyword analysis
# tdm (Term Document matrix)
#build corpus 코퍼스 생성
cps <- Corpus(VectorSource(df$ptext))
#summary(cps)
inspect(cps[1000])
cps <- tm_map(cps, removePunctuation)
cps <- tm_map(cps, removeNumbers)
#cps <- tm_map(cps, stemDocument)
cps <- tm_map(cps, stripWhitespace)
#cps <- tm_map(cps, stemDocument, language = "english")
#remove potentially problematic symbols
#필요없는 문제 제거
toSpace <- content_transformer(function(x, pattern) { return (gsub(pattern, " ", x))})
cps <- tm_map(cps, toSpace, "-")
##################################################################################
#
# Create Document Term Matrix with Frequency Count
#
##################################################################################
#library(RWeka)
#library(tokenizers)
tdm <- TermDocumentMatrix(cps,
control=list(tokenize = en_tokenizer
, wordLengths=c(2,30))) # extract over 2 word
inspect(tdm)
#Document Term Matrix with TF-IDF Score
dtm <- DocumentTermMatrix(cps,
control = list(weighting = function(x) weightTfIdf(x, normalize = FALSE),
stopwords = TRUE,
removePunctuation=T, # Remove Punctuation
removeNumbers=T, # Remove Number
wordLengths=c(2, Inf)
))
row.names(x_token) <- c("seq","token")
row.names(x_token) <- c("token")
names(x_token) <- c("token")
View(x_token)
x_token[,1]
en_tokenizer = function(x){
x <- as.character(x)
x_token = tokenize_words(x, stopwords = myStopwords)
x_token = as.data.frame(x_token)
return(x_token[,1])
}
en_tokenizer(df$ptext[1])
tdm <- TermDocumentMatrix(cps,
control=list(tokenize = en_tokenizer
, wordLengths=c(2,30))) # extract over 2 word
tdm$dimnames$Terms
myStopwords <- c(stopwords('en'),"rt","the","it","is","this","would","the",
"i","one","of","or","be","and","did","now","going","has","could","aa","aaa")
en_tokenizer = function(x){
x <- as.character(x)
x_token = tokenize_words(x, stopwords = myStopwords)
x_token = as.data.frame(x_token)
return(x_token[,1])
}
tdm <- TermDocumentMatrix(cps,
control=list(tokenize = en_tokenizer
, wordLengths=c(2,8))) # extract over 2 word
tdm$dimnames$Terms
myStopwords <- c(stopwords('en'),"rt","the","it","is","this","would","the",
"i","one","of","or","be","and","did","now","going","has","could","aa","aaa",
"aaaa","aan","aand","aantal","aaronps","aaronua","ab"
)
tdm <- TermDocumentMatrix(cps,
control=list(tokenize = en_tokenizer
, wordLengths=c(2,8))) # extract over 2 word
tdm$dimnames$Terms
tdm <- TermDocumentMatrix(cps,
control=list(tokenize = en_tokenizer
, wordLengths=c(3,8))) # extract over 2 word
tdm$dimnames$Terms
tdm2<-removeSparseTerms(tdm,sparse=0.9)
m2 <- as.matrix(tdm2)
t_m2 <- t(m2)
t_m2
tdm2
tdm2<-removeSparseTerms(tdm,sparse=0.95)
tdm2
tdm2<-removeSparseTerms(tdm,sparse=0.99)
m2 <- as.matrix(tdm2)
m2
tdm2
m2 <- as.matrix(tdm2)
t_m2 <- t(m2)
t_m2
rules2 <- apriori(t_m2, parameter = list(support = .3))
library("arules")
rules2 <- apriori(t_m2, parameter = list(support = .3))
aresult <- as(rules2, "data.frame")
inspect(head(sort(rules2, by = "lift"), 1000))
rules2 <- apriori(t_m2, parameter = list(support = .3))
inspect(head(sort(rules2, by = "lift"), 1000))
rules2 <- apriori(t_m2, parameter = list(support = .2))
inspect(head(sort(rules2, by = "lift"), 1000))
