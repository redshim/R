wordcloud(words=names(wordFreq_F),freq=wordFreq_F
,min.freq=1 ,random.order=F ,rot.per=.05 ,colors=pal ,family="malgun")
wordcloud(words=names(wordFreq_F),freq=wordFreq_F
,min.freq=1 ,random.order=F ,rot.per=.2 ,colors=pal ,family="malgun")
wordcloud(words=names(wordFreq_F),freq=wordFreq_F
,min.freq=1 ,random.order=T ,rot.per=1 ,colors=pal ,family="malgun")
wordcloud(words=names(wordFreq_F),freq=wordFreq_F
,min.freq=1 ,random.order=T ,rot.per=.5 ,colors=pal ,family="malgun")
wordcloud(words=names(wordFreq_F),freq=wordFreq_F
,min.freq=1 ,random.order=F ,rot.per=.5 ,colors=pal ,family="malgun")
library(httr)
library(rvest)
res = GET('http://cran.r-project.org/web/packages/httr/httr.pdf')
writeBin(content(res, 'raw'), 'httr.pdf')
h = html('http://gae9.com/trend/1DRlnSN7k1nb#!hot')
html_nodes(h, 'div.trend-post-content img')
html('http://gae9.com/trend/1DRlnSN7k1nb#!hot')
data(crude)
dtm <- DocumentTermMatrix(crude,
control = list(weighting = function(x) weightTfIdf(x, normalize = FALSE),
stopwords = TRUE))
dtm
inspect(dtm)
t(dtm)
tdm_c <- t(dtm)
tdm_c
t(tdm_c)
inspect(tdm_c)
crude
crude[1]
insepect(crude[1])
inspect(crude[1])
inspect(crude[1,1])
cps
dtm <- DocumentTermMatrix(cps,
control = list(weighting = function(x) weightTfIdf(x, normalize = FALSE),
stopwords = TRUE,
tokenize=ko.words, # 명사, 용언만 추출
removePunctuation=T, # 문장 부호 제거
removeNumbers=T, # 숫자 제거
wordLengths=c(2, 5)
))
dtm
tdm <- t(dtm)
inspect(tdm)
word.occur = as.array(rollup(tdm, 2))
length(word.occur)
word.occur
tdm_tf <- t(dtm)
m_tf <- as.matrix(tdm_tf)
wordFreq <- sort(rowSums(m_tf),decreasing=TRUE)
wordFreq[1:20]
wordFreq_F <- subset(wordFreq, wordFreq < 300)
wordFreq_F <- subset(wordFreq_F, wordFreq_F >=10)
wordFreq_F[1:20]
length(wordFreq_F)
set.seed(length(wordFreq_F))
pal <- brewer.pal(8,"Dark2")
windowsFonts(malgun=windowsFont("맑은 고딕"))
wordcloud(words=names(wordFreq_F),freq=wordFreq_F
,min.freq=1 ,random.order=F ,rot.per=.5 ,colors=pal ,family="malgun")
wordcloud(words=names(wordFreq_F),freq=wordFreq_F
,min.freq=2 ,random.order=F ,rot.per=.1 ,colors=pal ,family="malgun")
wordFreq_F <- subset(wordFreq_F, wordFreq_F >=30)
set.seed(50)
pal <- brewer.pal(8,"Dark2")
windowsFonts(malgun=windowsFont("맑은 고딕"))
wordcloud(words=names(wordFreq_F),freq=wordFreq_F
,min.freq=2 ,random.order=F ,rot.per=.1 ,colors=pal ,family="malgun")
set.seed(100)
pal <- brewer.pal(8,"Dark2")
windowsFonts(malgun=windowsFont("맑은 고딕"))
wordcloud(words=names(wordFreq_F),freq=wordFreq_F
,min.freq=2 ,random.order=F ,rot.per=.1 ,colors=pal ,family="malgun")
length(wordFreq_F)
wordFreq_F <- subset(wordFreq, wordFreq < 300)
wordFreq_F <- subset(wordFreq_F, wordFreq_F >=20)
length(wordFreq_F)
set.seed(80)
pal <- brewer.pal(8,"Dark2")
windowsFonts(malgun=windowsFont("맑은 고딕"))
wordcloud(words=names(wordFreq_F),freq=wordFreq_F
,min.freq=2 ,random.order=F ,rot.per=.1 ,colors=pal ,family="malgun")
paste(SimplePos09('하이브리드는 다 좋은데 갑자기 엔진이 돌 때 깜짝 놀랍니다'))
paste(SimplePos22('하이브리드는 다 좋은데 갑자기 엔진이 돌 때 깜짝 놀랍니다'))
extractNoun('하이브리드는 다 좋은데 갑자기 엔진이 돌 때 깜짝 놀랍니다')
gc()
extractNoun('하이브리드는 다 좋은데 갑자기 엔진이 돌 때 깜짝 놀랍니다')
a <- c('하이브리드는 다 좋은데 갑자기 엔진이 돌 때 깜짝 놀랍니다')
extractNoun(a)
useSejongDic()
extractNoun(a)
cust_id <- c("c01","c02","c03","c04")
last_name <- c("Kim", "Lee", "Choi", "Park")
cust_mart_1 <- data.frame(cust_id, last_name)
View(cust_mart_1)
cust_mart_1
cust_mart_2 <- data.frame(cust_id = c("c05", "c06", "c07"),
+          last_name = c("Bae", "Kim", "Lim"))
cust_mart_2
cust_mart_2 <- data.frame(cust_id = c("c05", "c06", "c07")
, last_name = c("Bae", "Kim", "Lim"))
cust_mart_2
cust_mart_12 <- rbind(cust_mart_1, cust_mart_2)
View(cust_mart_12)
cust_mart_3 <- data.frame(cust_id = c("c08", "c09"),
last_name = c("Lee", "Park"),
gender = c("F", "M"))
cust_mart_3
rbind(cust_mart_12, cust_mart_3)
txt <-
c("Colors and odors are associated; for instance, people typically match the smell of strawberries to the color pink or red."
,"These associations are forms of crossmodal correspondences. Recently, there has been discussion about the extent to which these correspondences"
,"arise for structural reasons (i.e., an inherent mapping between color and odor), statistical reasons "
,"(i.e., covariance in experience), and/or semantically-mediated reasons (i.e., stemming from language)."
,"study probed this question by testing color-odor correspondences in 6 different cultural groups "
,"(Dutch, Netherlands-residing-Chinese, German, Malay, Malaysian-Chinese, and US residents")
nDocs <- length(txt)
nDocs
txt
df <- do.call("rbind", lapply(txt, as.data.frame))
df
View(df)
as.data.frame(txt)
txt1 <- as.data.frame(txt)
View(txt1)
removeDoc <- function(x) {gsub("@[[:graph:]]*", "", x)}
removeURL <- function(x) { gsub("http://[[:graph:]]*", "", x)}
trim <- function (x) {gsub("^\\s+|\\s+$", "", x)}
removenum <- function (x) {gsub("\\d+", "", x)}
library(tm)
myCorpus_ <- Corpus(VectorSource(df$ptext))
myCorpus_ <- tm_map(myCorpus_, removePunctuation)
myCorpus_ <- tm_map(myCorpus_, removeNumbers)
myCorpus_ <- tm_map(myCorpus_, stemDocument)
myCorpus_ <- tm_map(myCorpus_, stripWhitespace)
myCorpus_ <- tm_map(myCorpus_, stemDocument, language = "english")
df$ptext <- sapply(df$X, removeURL)
df$ptext <- sapply(df$ptext, removeDoc)
df$ptext <- sapply(df$ptext, trim)
df$ptext <- sapply(df$ptext, removenum)
library(tm)
View(df)
myCorpus_ <- Corpus(VectorSource(df$ptext))
myCorpus_ <- tm_map(myCorpus_, removePunctuation)
myCorpus_ <- tm_map(myCorpus_, removeNumbers)
myCorpus_ <- tm_map(myCorpus_, stemDocument)
myCorpus_ <- tm_map(myCorpus_, stripWhitespace)
myCorpus_ <- tm_map(myCorpus_, stemDocument, language = "english")
myCorpus_
inspect(myCorpus_)
myTdm <- TermDocumentMatrix(myCorpus_, control=list(wordLengths=c(2,Inf)))
as.matrix(myTdm)
t(as.matrix(myTdm))
dat = read.csv('magic_body_control.csv'
, header=T
, fileEncoding = "utf8"
, sep = ","
, quote = "\""
, stringsAsFactors = F)
setwd("C://R//TextAnalysis//German")
dat = read.csv('magic_body_control.csv'
, header=T
, fileEncoding = "utf8"
, sep = ","
, quote = "\""
, stringsAsFactors = F)
Sys.setlocale(category = "LC_ALL", locale = "German")
dat = read.csv('magic_body_control.csv'
, header=T
, fileEncoding = "utf8"
, sep = ","
, quote = "\""
, stringsAsFactors = F)
Sys.setenv(JAVA_HOME="")  # 충돌 방지
library(coreNLP)
initCoreNLP(type='german')
doc = dat$Text[1]
doc
output = annotateString(doc)
result = getToken(output)
result
result[grep('ADJA|NN|^V', result$POS), c('token', 'POS')]
ger_tokenizer = function(doc){
doc <- as.character(doc)
output = annotateString(doc)
result = getToken(output)
result[grep('ADJA|NN|^V', result$POS),'token']
}
dat$Text[3]
ger_tokenizer(dat$Text[3])
library(tm)
cps <- Corpus(VectorSource(dat$Text))
cps <- tm_map(cps, content_transformer(tolower)) # 대문자 → 소문자
cps <- tm_map(cps, removeWords, stopwords("german")) # 띄어쓰기와 시제 등의 내용 제거
cps <- tm_map(cps, content_transformer(tolower)) # 대문자 → 소문자
cps <- tm_map(cps, removeWords, stopwords("german")) # 띄어쓰기와 시제 등의 내용 제거
cps <- tm_map(cps, removePunctuation)
cps <- tm_map(cps, removeNumbers)
cps <- tm_map(cps, stripWhitespace)
myStopwords <- c(stopwords('german'), "rt","like","will","just","the")
cps <-tm_map(cps, removeWords, myStopwords) # 사용자 지정 Stopword 제거
myTdm <- TermDocumentMatrix(cps
, control=list(wordLengths=c(3,6)))
myTdm <- removeSparseTerms(myTdm, 0.995)
library(slam)
word.count = as.array(rollup(myTdm, 2)) # 단어가 전체 문서에서 쓰인 개수
word.count
word.order = order(word.count, decreasing = T)
freq.word = word.order[1:30]
word <- myTdm$dimnames$Terms[freq.word]
freq <- word.count[freq.word]
# csv 파일로 주요 키워드 횟수 저장
top.freq = data.frame(word=myTdm$dimnames$Terms[freq.word], freq=word.count[freq.word])
top.freq
library(ggplot2)
library(ggplot2)
library(plyr)
library(reshape2)
gg1 <-data.frame(term = word, frequency=freq, order(freq, decreasing = TRUE))
Q <- ggplot(gg1, aes(term,frequency), colour = 'red', size = 3)
#Q + geom_bar(stat="identity")
#Q + geom_bar(colour="blue", stat="identity") + guides(fill=FALSE)
Q +
geom_bar(fill="gray", colour="darkgray", stat="identity", width=.8)   +
coord_flip()   +
scale_fill_brewer(palette="Dark2") +
geom_text(aes(label=freq),color="black", size=3.5) +
theme_minimal() +
ggtitle("Term Frequency Plot") +
theme(plot.title = element_text(lineheight=.5, face="bold",margin=margin(0,0,0,0)))
library(wordcloud)
m <- as.matrix(myTdm)
wordFreq <- sort(rowSums(m),decreasing=TRUE)
head(wordFreq)
wordFreq_F <- subset(wordFreq, wordFreq >50)
#wordFreq_F
wordFreq_F <- subset(wordFreq_F, wordFreq_F <150)
length(wordFreq_F)
#wordFreq_F
set.seed(length(wordFreq_F))
pal <- brewer.pal(8,"Dark2")
windowsFonts(malgun=windowsFont("맑은 고딕"))
wordcloud(words=names(wordFreq_F),freq=wordFreq_F
,min.freq=2 ,random.order=F ,rot.per=.1 ,colors=pal ,family="malgun")
library(wordcloud2)
m1 <- as.matrix(myTdm)
v <- sort(rowSums(m1), decreasing = TRUE)[1:100]
d <- data.frame(word = names(v), freq = v)
wordcloud2(d, size=2) # size로 크기 조절
setwd("C://R//TextAnalysis//German")
getwd()
Sys.setlocale(category = "LC_ALL", locale = "German")
dat = read.csv('magic_body_control.csv'
, header=T
, fileEncoding = "utf8"
, sep = ","
, quote = "\""
, stringsAsFactors = F)
Sys.setenv(JAVA_HOME="")  # 충돌 방지
library(coreNLP)
initCoreNLP(type='german')
dat$Text[10]
dat$Text[1]
doc = dat$Text[1]
output = annotateString(doc)
output
output
result = getToken(output)
result
result[grep('ADJA|NN|^V', result$POS), c('token', 'POS')]
ger_tokenizer = function(doc){
doc <- as.character(doc)
output = annotateString(doc)
result = getToken(output)
result[grep('ADJA|NN|^V', result$POS),'token']
}
ger_tokenizer(doc)
library(tm)
cps <- Corpus(VectorSource(dat$Text))
cps
cps <- tm_map(cps, content_transformer(tolower)) # Upper Case →  LowerCase
cps <- tm_map(cps, removeWords, stopwords("german")) # 띄어쓰기와 시제 등의 내용 제거
cps <- tm_map(cps, removePunctuation)
cps <- tm_map(cps, removeNumbers)
cps <- tm_map(cps, stripWhitespace)
myStopwords <- c(stopwords('german'), "rt","like","will","just","the")
cps <-tm_map(cps, removeWords, myStopwords) # 사용자 지정 Stopword 제거
myTdm <- TermDocumentMatrix(cps
, control=list(wordLengths=c(3,6)))
myTdm
myTdm <- removeSparseTerms(myTdm, 0.995)
myTdm
library(slam)
word.count = as.array(rollup(myTdm, 2)) # 단어가 전체 문서에서 쓰인 개수
word.count
word.order = order(word.count, decreasing = T)
word.order
word.count
word.order
freq.word = word.order[1:30]
freq.word
word <- myTdm$dimnames$Terms[freq.word]
word
freq <- word.count[freq.word]
freq
top.freq = data.frame(word=myTdm$dimnames$Terms[freq.word], freq=word.count[freq.word])
top.freq
library(ggplot2)
library(plyr)
library(reshape2)
gg1 <-data.frame(term = word, frequency=freq, order(freq, decreasing = TRUE))
Q <- ggplot(gg1, aes(term,frequency), colour = 'red', size = 3)
#Q + geom_bar(stat="identity")
#Q + geom_bar(colour="blue", stat="identity") + guides(fill=FALSE)
Q +
geom_bar(fill="gray", colour="darkgray", stat="identity", width=.8)   +
coord_flip()   +
scale_fill_brewer(palette="Dark2") +
geom_text(aes(label=freq),color="black", size=3.5) +
theme_minimal() +
ggtitle("Term Frequency Plot") +
theme(plot.title = element_text(lineheight=.5, face="bold",margin=margin(0,0,0,0)))
findFreqTerms(myTdm, lowfreq=6)
findAssocs(myTdm,'sicher',0.7)
findAssocs(myTdm,'neue',0.6)
library(wordcloud)
m <- as.matrix(myTdm)
m
wordFreq <- sort(rowSums(m),decreasing=TRUE)
head(wordFreq)
wordFreq_F <- subset(wordFreq, wordFreq > 20)
wordFreq_F <- subset(wordFreq, wordFreq > 20)
#wordFreq_F
wordFreq_F <- subset(wordFreq_F, wordFreq_F < 156)
length(wordFreq_F)
wordFreq_F <- subset(wordFreq, wordFreq > 10)
#wordFreq_F
wordFreq_F <- subset(wordFreq_F, wordFreq_F < 156)
length(wordFreq_F)
wordFreq_F
set.seed(length(wordFreq_F))
pal <- brewer.pal(8,"Dark2")
windowsFonts(malgun=windowsFont("맑은 고딕"))
wordcloud(words=names(wordFreq_F),freq=wordFreq_F
,min.freq=2 ,random.order=F ,rot.per=.1 ,colors=pal ,family="malgun")
library(wordcloud2)
m1 <- as.matrix(myTdm)
sort(rowSums(m1), decreasing = TRUE)[1:100]
data.frame(word = names(v), freq = v)
v <- sort(rowSums(m1), decreasing = TRUE)[1:100]
data.frame(word = names(v), freq = v)
d <- data.frame(word = names(v), freq = v)
wordcloud2(d, size=2) # size로 크기 조절
most50 = word.order[1:50]
most50
myTdm.mat = as.matrix(myTdm[most50,])
myTdm.mat
t(myTdm.mat)
cormat = cor(t(myTdm.mat))
cormat
library(qgraph)
qgraph(cormat, labels=rownames(cormat)
, diag=F
, layout='spring'
, theme = 'Borkulo'
, label.scale=F
, label.font =1
, label.fill.horizontal = 1
)
most50 = word.order[1:80]
myTdm.mat = as.matrix(myTdm[most50,])
# Correlation Matrix
cormat = cor(t(myTdm.mat))
library(qgraph)
qgraph(cormat, labels=rownames(cormat)
, diag=F
, layout='spring'
, theme = 'Borkulo'
, label.scale=F
, label.font =1
, label.fill.horizontal = 1
)
title("Co-occourance words between the sentence", line = 2.5)
library(tm)
library(lda)
library(topicmodels)
dtm = as.DocumentTermMatrix(myTdm)
dtm
ldaform = dtm2ldaformat(dtm, omit_empty = T)
ldaform
attributs(ldaform)
attributes(ldaform)
result.lda = lda.collapsed.gibbs.sampler(documents = ldaform$documents,
K = 10,
vocab = ldaform$vocab,
num.iterations = 5000,
burnin = 1000,
alpha = 0.01,
eta = 0.01)
attributes(result.lda)
result.lda$assignments
result.lda$topics
top.topic.words(result.lda$topics)[1:10,]
result.lda$topic_sums
result.lda$document_sums
alpha <- 0.02
eta <- 0.02
D <- length(ldaform$documents)  # number of documents (2,000)
W <- length(ldaform$vocab)  # number of terms in the vocab (14,568)
doc.length <- sapply(ldaform$documents, function(x) sum(x[2, ]))  # number of tokens per document [312, 288, 170, 436, 291, ...]
N <- sum(doc.length)  # total number of tokens in the data (546,827)
term.table <- table(unlist(word.order))
term.table <- sort(term.table, decreasing = TRUE)
term.frequency <- as.integer(term.table)  # frequencies of terms in the corpus [8939, 5544, 2411, 2410, 2143, ...]
theta <- t(apply(result.lda$document_sums + alpha, 2, function(x) x/sum(x)))
phi <- t(apply(t(result.lda$topics) + eta, 2, function(x) x/sum(x)))
CustComplaints <- list(phi = phi,
theta = theta,
doc.length = doc.length,
vocab = ldaform$vocab,
term.frequency = term.frequency)
library(LDAvis)
library(servr)
json <- createJSON(phi = CustComplaints$phi,
theta = CustComplaints$theta,
doc.length = CustComplaints$doc.length,
vocab = CustComplaints$vocab,
term.frequency = CustComplaints$term.frequency,
fileEncoding="utf8",
encoding = "utf8",
family= "Nanumgothic")
cat(json, file=file('vis4/lda.json', encoding='utf8'))
serVis(json, out.dir = 'vis4', open.browser = FALSE, encoding = "UTF-8", family= "Nanumgothic")
serVis(json, out.dir = 'vis1', open.browser = FALSE, encoding = "UTF-8", family= "Nanumgothic")
cat(json, file=file('vis1/lda.json', encoding='utf8'))
library(tm)
library(lda)
library(topicmodels)
dtm = as.DocumentTermMatrix(myTdm)
ldaform = dtm2ldaformat(dtm, omit_empty = T)
result.lda = lda.collapsed.gibbs.sampler(documents = ldaform$documents,
K = 4,
vocab = ldaform$vocab,
num.iterations = 5000,
burnin = 1000,
alpha = 0.01,
eta = 0.01)
alpha <- 0.02
eta <- 0.02
D <- length(ldaform$documents)  # number of documents (2,000)
W <- length(ldaform$vocab)  # number of terms in the vocab (14,568)
doc.length <- sapply(ldaform$documents, function(x) sum(x[2, ]))  # number of tokens per document [312, 288, 170, 436, 291, ...]
N <- sum(doc.length)  # total number of tokens in the data (546,827)
term.table <- table(unlist(word.order))
term.table <- sort(term.table, decreasing = TRUE)
term.frequency <- as.integer(term.table)  # frequencies of terms in the corpus [8939, 5544, 2411, 2410, 2143, ...]
theta <- t(apply(result.lda$document_sums + alpha, 2, function(x) x/sum(x)))
phi <- t(apply(t(result.lda$topics) + eta, 2, function(x) x/sum(x)))
#
CustComplaints <- list(phi = phi,
theta = theta,
doc.length = doc.length,
vocab = ldaform$vocab,
term.frequency = term.frequency)
json <- createJSON(phi = CustComplaints$phi,
theta = CustComplaints$theta,
doc.length = CustComplaints$doc.length,
vocab = CustComplaints$vocab,
term.frequency = CustComplaints$term.frequency,
fileEncoding="utf8",
encoding = "utf8",
family= "Nanumgothic")
serVis(json, out.dir = 'vis4', open.browser = FALSE, encoding = "UTF-8", family= "Nanumgothic")
cat(json, file=file('vis4/lda.json', encoding='utf8'))
serVis(json, out.dir = 'vis4', open.browser = interactive(),  fileEncoding = "utf8", Encoding ="utf8")
motortalk = c("http://search.motortalk.net/?area=1&query=motion+body")
board = read_html(motortalk[1])
library(httr)
library(dplyr)
library(rvest)
board = read_html(motortalk[1])
posts = board %>%
html_nodes('.result-inner') %>%
html_nodes('.title')  %>%
html_attr('href')
posts
board = read_html(motortalk)
board
#
posts = board %>%
html_nodes('.result-inner') %>%
html_nodes('.title')  %>%
html_attr('href')
posts
set.seed(length(wordFreq_F))
pal <- brewer.pal(8,"Dark2")
windowsFonts(malgun=windowsFont("맑은 고딕"))
wordcloud(words=names(wordFreq_F),freq=wordFreq_F
,min.freq=2 ,random.order=F ,rot.per=.1 ,colors=pal ,family="malgun")
# Visualization
library(qgraph)
qgraph(cormat, labels=rownames(cormat)
, diag=F
, layout='spring'
, theme = 'Borkulo'
, label.scale=F
, label.font =1
, label.fill.horizontal = 1
)
title("Co-occourance words between the sentence", line = 2.5)
serVis(json, out.dir = 'vis4', open.browser = interactive(),  fileEncoding = "utf8", Encoding ="utf8")
