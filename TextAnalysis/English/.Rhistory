set.seed(length(wordFreq_F))
pal <- brewer.pal(8,"Dark2")
windowsFonts(malgun=windowsFont("맑은 고딕"))
wordcloud(words=names(wordFreq_F),freq=wordFreq_F
,min.freq=1 ,random.order=F ,rot.per=.05 ,colors=pal ,family="malgun")
wordFreq_F <- subset(wordFreq_F, wordFreq_F >=10)
wordFreq_F[1:20]
set.seed(length(wordFreq_F))
pal <- brewer.pal(8,"Dark2")
windowsFonts(malgun=windowsFont("맑은 고딕"))
wordcloud(words=names(wordFreq_F),freq=wordFreq_F
,min.freq=1 ,random.order=F ,rot.per=.05 ,colors=pal ,family="malgun")
wordcloud(words=names(wordFreq_F),freq=wordFreq_F
,min.freq=1 ,random.order=F ,rot.per=.2 ,colors=pal ,family="malgun")
wordcloud(words=names(wordFreq_F),freq=wordFreq_F
,min.freq=1 ,random.order=T ,rot.per=1 ,colors=pal ,family="malgun")
wordcloud(words=names(wordFreq_F),freq=wordFreq_F
,min.freq=1 ,random.order=T ,rot.per=.5 ,colors=pal ,family="malgun")
wordcloud(words=names(wordFreq_F),freq=wordFreq_F
,min.freq=1 ,random.order=F ,rot.per=.5 ,colors=pal ,family="malgun")
library(httr)
library(rvest)
res = GET('http://cran.r-project.org/web/packages/httr/httr.pdf')
writeBin(content(res, 'raw'), 'httr.pdf')
h = html('http://gae9.com/trend/1DRlnSN7k1nb#!hot')
html_nodes(h, 'div.trend-post-content img')
html('http://gae9.com/trend/1DRlnSN7k1nb#!hot')
data(crude)
dtm <- DocumentTermMatrix(crude,
control = list(weighting = function(x) weightTfIdf(x, normalize = FALSE),
stopwords = TRUE))
dtm
inspect(dtm)
t(dtm)
tdm_c <- t(dtm)
tdm_c
t(tdm_c)
inspect(tdm_c)
crude
crude[1]
insepect(crude[1])
inspect(crude[1])
inspect(crude[1,1])
cps
dtm <- DocumentTermMatrix(cps,
control = list(weighting = function(x) weightTfIdf(x, normalize = FALSE),
stopwords = TRUE,
tokenize=ko.words, # 명사, 용언만 추출
removePunctuation=T, # 문장 부호 제거
removeNumbers=T, # 숫자 제거
wordLengths=c(2, 5)
))
dtm
tdm <- t(dtm)
inspect(tdm)
word.occur = as.array(rollup(tdm, 2))
length(word.occur)
word.occur
tdm_tf <- t(dtm)
m_tf <- as.matrix(tdm_tf)
wordFreq <- sort(rowSums(m_tf),decreasing=TRUE)
wordFreq[1:20]
wordFreq_F <- subset(wordFreq, wordFreq < 300)
wordFreq_F <- subset(wordFreq_F, wordFreq_F >=10)
wordFreq_F[1:20]
length(wordFreq_F)
set.seed(length(wordFreq_F))
pal <- brewer.pal(8,"Dark2")
windowsFonts(malgun=windowsFont("맑은 고딕"))
wordcloud(words=names(wordFreq_F),freq=wordFreq_F
,min.freq=1 ,random.order=F ,rot.per=.5 ,colors=pal ,family="malgun")
wordcloud(words=names(wordFreq_F),freq=wordFreq_F
,min.freq=2 ,random.order=F ,rot.per=.1 ,colors=pal ,family="malgun")
wordFreq_F <- subset(wordFreq_F, wordFreq_F >=30)
set.seed(50)
pal <- brewer.pal(8,"Dark2")
windowsFonts(malgun=windowsFont("맑은 고딕"))
wordcloud(words=names(wordFreq_F),freq=wordFreq_F
,min.freq=2 ,random.order=F ,rot.per=.1 ,colors=pal ,family="malgun")
set.seed(100)
pal <- brewer.pal(8,"Dark2")
windowsFonts(malgun=windowsFont("맑은 고딕"))
wordcloud(words=names(wordFreq_F),freq=wordFreq_F
,min.freq=2 ,random.order=F ,rot.per=.1 ,colors=pal ,family="malgun")
length(wordFreq_F)
wordFreq_F <- subset(wordFreq, wordFreq < 300)
wordFreq_F <- subset(wordFreq_F, wordFreq_F >=20)
length(wordFreq_F)
set.seed(80)
pal <- brewer.pal(8,"Dark2")
windowsFonts(malgun=windowsFont("맑은 고딕"))
wordcloud(words=names(wordFreq_F),freq=wordFreq_F
,min.freq=2 ,random.order=F ,rot.per=.1 ,colors=pal ,family="malgun")
paste(SimplePos09('하이브리드는 다 좋은데 갑자기 엔진이 돌 때 깜짝 놀랍니다'))
paste(SimplePos22('하이브리드는 다 좋은데 갑자기 엔진이 돌 때 깜짝 놀랍니다'))
extractNoun('하이브리드는 다 좋은데 갑자기 엔진이 돌 때 깜짝 놀랍니다')
gc()
extractNoun('하이브리드는 다 좋은데 갑자기 엔진이 돌 때 깜짝 놀랍니다')
a <- c('하이브리드는 다 좋은데 갑자기 엔진이 돌 때 깜짝 놀랍니다')
extractNoun(a)
useSejongDic()
extractNoun(a)
cust_id <- c("c01","c02","c03","c04")
last_name <- c("Kim", "Lee", "Choi", "Park")
cust_mart_1 <- data.frame(cust_id, last_name)
View(cust_mart_1)
cust_mart_1
cust_mart_2 <- data.frame(cust_id = c("c05", "c06", "c07"),
+          last_name = c("Bae", "Kim", "Lim"))
cust_mart_2
cust_mart_2 <- data.frame(cust_id = c("c05", "c06", "c07")
, last_name = c("Bae", "Kim", "Lim"))
cust_mart_2
cust_mart_12 <- rbind(cust_mart_1, cust_mart_2)
View(cust_mart_12)
cust_mart_3 <- data.frame(cust_id = c("c08", "c09"),
last_name = c("Lee", "Park"),
gender = c("F", "M"))
cust_mart_3
rbind(cust_mart_12, cust_mart_3)
txt <-
c("Colors and odors are associated; for instance, people typically match the smell of strawberries to the color pink or red."
,"These associations are forms of crossmodal correspondences. Recently, there has been discussion about the extent to which these correspondences"
,"arise for structural reasons (i.e., an inherent mapping between color and odor), statistical reasons "
,"(i.e., covariance in experience), and/or semantically-mediated reasons (i.e., stemming from language)."
,"study probed this question by testing color-odor correspondences in 6 different cultural groups "
,"(Dutch, Netherlands-residing-Chinese, German, Malay, Malaysian-Chinese, and US residents")
nDocs <- length(txt)
nDocs
txt
df <- do.call("rbind", lapply(txt, as.data.frame))
df
View(df)
as.data.frame(txt)
txt1 <- as.data.frame(txt)
View(txt1)
removeDoc <- function(x) {gsub("@[[:graph:]]*", "", x)}
removeURL <- function(x) { gsub("http://[[:graph:]]*", "", x)}
trim <- function (x) {gsub("^\\s+|\\s+$", "", x)}
removenum <- function (x) {gsub("\\d+", "", x)}
library(tm)
myCorpus_ <- Corpus(VectorSource(df$ptext))
myCorpus_ <- tm_map(myCorpus_, removePunctuation)
myCorpus_ <- tm_map(myCorpus_, removeNumbers)
myCorpus_ <- tm_map(myCorpus_, stemDocument)
myCorpus_ <- tm_map(myCorpus_, stripWhitespace)
myCorpus_ <- tm_map(myCorpus_, stemDocument, language = "english")
df$ptext <- sapply(df$X, removeURL)
df$ptext <- sapply(df$ptext, removeDoc)
df$ptext <- sapply(df$ptext, trim)
df$ptext <- sapply(df$ptext, removenum)
library(tm)
View(df)
myCorpus_ <- Corpus(VectorSource(df$ptext))
myCorpus_ <- tm_map(myCorpus_, removePunctuation)
myCorpus_ <- tm_map(myCorpus_, removeNumbers)
myCorpus_ <- tm_map(myCorpus_, stemDocument)
myCorpus_ <- tm_map(myCorpus_, stripWhitespace)
myCorpus_ <- tm_map(myCorpus_, stemDocument, language = "english")
myCorpus_
inspect(myCorpus_)
myTdm <- TermDocumentMatrix(myCorpus_, control=list(wordLengths=c(2,Inf)))
as.matrix(myTdm)
t(as.matrix(myTdm))
library(Rwordseg)
tdm <- TermDocumentMatrix(cps,
control=list(tokenize=chs_tokenizer,
wordLengths=c(2,Inf), # extract over 2 word
stopwords= TRUE)
) # delet stopwords
library(tm)
cps <- Corpus(VectorSource(a_conv[1:100])) # select 100 items
d = read.csv2('sail_review_100_conv.csv'
, header=TRUE
, fileEncoding='utf8'
, sep = ','
, quote = '"'
, stringsAsFactors = F)
d <- as.data.frame(d)
# d <- read.csv('data_chinese.csv', header=T, fileEncoding='utf8', sep = ',', quote = '"', stringsAsFactors = F)
View(d)
# Remove none related text
library(stringr)
a <- d$contents
##gsub("\\[.*\\]"," ","【最满意的一点】")
gsub("【.*?】" # find pattern
," " # changed character
,"【最满意的一点】_print_test_【最满意的一点】_test_【最满意的一点】") #input
a <- gsub("【.*?】","", a)
a <- a[nchar(a) > 0]
a_conv <- cbind(as.character(a))
class(a_conv)
names(a_conv) <- c("contents")
d = read.csv2('sail_review_100_conv.csv'
, header=TRUE
, fileEncoding='utf8'
, sep = ','
, quote = '"'
, stringsAsFactors = F)
setwd("C://R//TextAnalysis//Chinese - Training")
d = read.csv2('sail_review_100_conv.csv'
, header=TRUE
, fileEncoding='utf8'
, sep = ','
, quote = '"'
, stringsAsFactors = F)
d <- as.data.frame(d)
View(d)
d <- read_excel("sail_review_100.xlsx",skip = 1)
writeLines(as.character(d$contents[[1]]))
View(d)
# locale change
original.locale = Sys.getlocale('LC_CTYPE')
Sys.setlocale(category = "LC_ALL", locale = "Chinese_China.936")
d <- read_excel("sail_review_100.xlsx",skip = 1)
library(readxl)
d <- read_excel("sail_review_100.xlsx",skip = 1)
# tokenizer
chs_tokenizer = function(doc){
doc <- as.character(doc)
a = segmentCN(doc, nature = T, nosymbol = T)
dat = matrix(cbind(a, names(a)), ncol=2)
return(dat[grep('^[vna]+', dat[,2]),1])
}
library(tm)
cps <- Corpus(VectorSource(a_conv[1:100])) # select 100 items
library(stringr)
a <- d$contents
##gsub("\\[.*\\]"," ","【最满意的一点】")
gsub("【.*?】" # find pattern
," " # changed character
,"【最满意的一点】_print_test_【最满意的一点】_test_【最满意的一点】") #input
a <- gsub("【.*?】","", a)
a <- a[nchar(a) > 0]
a_conv <- cbind(as.character(a))
class(a_conv)
names(a_conv) <- c("contents")
library(tm)
cps <- Corpus(VectorSource(a_conv[1:100])) # select 100 items
cps <- Corpus(VectorSource(a_conv[1:100]),Encoding="utf8") # select 100 items
cps
inspect(cps)
tdm <- TermDocumentMatrix(cps,
control=list(tokenize=chs_tokenizer,
wordLengths=c(2,Inf), # extract over 2 word
stopwords= TRUE)
) # delet stopwords
inspect(tdm)
setwd("C:/R")
getwd()
d = read.csv2('data.txt'
, header=TRUE
, fileEncoding='utf8'
, sep = ','
, quote = '"'
, stringsAsFactors = F)
d = read.csv2('data.txt'
, header=TRUE
, fileEncoding='utf8'
, sep = ','
, quote = '"'
, stringsAsFactors = F)
d = read.csv2('data.txt'
, header=TRUE
# , fileEncoding='utf8'
, sep = ','
, quote = '"'
, stringsAsFactors = F)
View(d)
install.packages("SparkR")
Sys.getenv()
sparkR.session()
install.packages("SparkR")
install.packages("plotly")
library(plotly)
packageVersion('plotly')
data(karate, package="igraphdata")
G <- upgrade_graph(karate)
L <- layout.circle(G)
library(plotly)
library(igraph)
data(karate, package="igraphdata")
G <- upgrade_graph(karate)
L <- layout.circle(G)
library(igraph)
data(karate, package="igraphdata")
data(karate, package="igraph")
library(igraphdata)
install.packages("igraphdata")
library(igraphdata)
data(karate, package="igraphdata")
G <- upgrade_graph(karate)
L <- layout.circle(G)
vs <- V(G)
es <- as.data.frame(get.edgelist(G))
Nv <- length(vs)
Ne <- length(es[1]$V1)
Xn <- L[,1]
Yn <- L[,2]
network <- plot_ly(x = ~Xn, y = ~Yn, mode = "markers", text = vs$label, hoverinfo = "text")
edge_shapes <- list()
for(i in 1:Ne) {
v0 <- es[i,]$V1
v1 <- es[i,]$V2
edge_shape = list(
type = "line",
line = list(color = "#030303", width = 0.3),
x0 = Xn[v0],
y0 = Yn[v0],
x1 = Xn[v1],
y1 = Yn[v1]
)
edge_shapes[[i]] <- edge_shape
}
axis <- list(title = "", showgrid = FALSE, showticklabels = FALSE, zeroline = FALSE)
p <- layout(
network,
title = 'Karate Network',
shapes = edge_shapes,
xaxis = axis,
yaxis = axis
)
# Create a shareable link to your chart
# Set up API credentials: https://plot.ly/r/getting-started
chart_link = plotly_POST(p, filename="karate-network-r")
chart_link
page_ori = 'https://teslamotorsclub.com/tmc/threadloom/search?query=noise'
addr1 = 'https://teslamotorsclub.com/tmc/'
post_texts = c()
for(i in 1:2){
page = modify_url(page_ori, query=list(page=i))
board = read_html(page, encoding="UTF-8")
posts = board %>%
html_nodes('.threadloom-cardHeader') %>%
html_nodes('h1 > a')  %>%
html_attr('href') %>%
paste(addr1,.,sep = "")
posts
for(post_url in posts){
post = read_html(post_url) %>%
html_nodes('.messageContent') %>%
html_nodes('article > blockquote') %>%
html_text()  %>%
gsub("\n", "", .) %>%
gsub("\t", "", .)
post_texts = c(post_texts, post)
}
}
library(httr)
library(dplyr)
library(rvest)
page_ori = 'https://teslamotorsclub.com/tmc/threadloom/search?query=noise'
addr1 = 'https://teslamotorsclub.com/tmc/'
post_texts = c()
for(i in 1:2){
page = modify_url(page_ori, query=list(page=i))
board = read_html(page, encoding="UTF-8")
posts = board %>%
html_nodes('.threadloom-cardHeader') %>%
html_nodes('h1 > a')  %>%
html_attr('href') %>%
paste(addr1,.,sep = "")
posts
for(post_url in posts){
post = read_html(post_url) %>%
html_nodes('.messageContent') %>%
html_nodes('article > blockquote') %>%
html_text()  %>%
gsub("\n", "", .) %>%
gsub("\t", "", .)
post_texts = c(post_texts, post)
}
}
post_texts
post_df = as.data.frame(post_texts)
View(post_df)
View(post_df)
inspect(tdm)
inspect(tdm)
library(tm)
inspect(tdm)
inspect(tdm)
tdm
cps
library(httr)
library(dplyr)
library(rvest)
page_ori = 'https://teslamotorsclub.com/tmc/threadloom/search?query=noise'
addr1 = 'https://teslamotorsclub.com/tmc/'
post_texts = c()
for(i in 1:2){
page = modify_url(page_ori, query=list(page=i))
board = read_html(page, encoding="UTF-8")
posts = board %>%
html_nodes('.threadloom-cardHeader') %>%
html_nodes('h1 > a')  %>%
html_attr('href') %>%
paste(addr1,.,sep = "")
posts
for(post_url in posts){
post = read_html(post_url) %>%
html_nodes('.messageContent') %>%
html_nodes('article > blockquote') %>%
html_text()  %>%
gsub("\n", "", .) %>%
gsub("\t", "", .)
post_texts = c(post_texts, post)
}
}
library(data.table)
library(tm)
library(wordcloud2)
library(ggplot2)
library(qgraph)
library(SnowballC)
library(readxl)
setwd("C://R//TextAnalysis//English")
library(tm)
cps <- Corpus(VectorSource(post_texts))
cps <- tm_map(cps, removePunctuation)
cps <- tm_map(cps, removeNumbers)
cps <- tm_map(cps, stemDocument)
cps <- tm_map(cps, stripWhitespace)
cps <- tm_map(cps, stemDocument, language = "english")
cps
tdm <- TermDocumentMatrix(cps,
control=list(wordLengths=c(2,Inf))) # extract over 2 word
tdm
inspect(tdm)
#Document Term Matrix with TF-IDF Score
dtm <- DocumentTermMatrix(cps,
control = list(weighting = function(x) weightTfIdf(x, normalize = FALSE),
stopwords = TRUE,
removePunctuation=T, # Remove Punctuation
removeNumbers=T, # Remove Number
wordLengths=c(2, Inf)
))
tdm_tf <- t(dtm)
library(slam)
word.count = as.array(rollup(tdm, 2)) # Rolling up group by the term
tdm
page_ori = 'https://teslamotorsclub.com/tmc/threadloom/search?query=noise'
addr1 = 'https://teslamotorsclub.com/tmc/'
post_texts = c()
for(i in 1:10){
page = modify_url(page_ori, query=list(page=i))
board = read_html(page, encoding="UTF-8")
posts = board %>%
html_nodes('.threadloom-cardHeader') %>%
html_nodes('h1 > a')  %>%
html_attr('href') %>%
paste(addr1,.,sep = "")
posts
for(post_url in posts){
post = read_html(post_url) %>%
html_nodes('.messageContent') %>%
html_nodes('article > blockquote') %>%
html_text()  %>%
gsub("\n", "", .) %>%
gsub("\t", "", .)
post_texts = c(post_texts, post)
}
}
library(httr)
library(dplyr)
library(rvest)
page_ori = 'https://teslamotorsclub.com/tmc/threadloom/search?query=noise'
addr1 = 'https://teslamotorsclub.com/tmc/'
post_texts = c()
for(i in 1:10){
page = modify_url(page_ori, query=list(page=i))
board = read_html(page, encoding="UTF-8")
posts = board %>%
html_nodes('.threadloom-cardHeader') %>%
html_nodes('h1 > a')  %>%
html_attr('href') %>%
paste(addr1,.,sep = "")
posts
for(post_url in posts){
post = read_html(post_url) %>%
html_nodes('.messageContent') %>%
html_nodes('article > blockquote') %>%
html_text()  %>%
gsub("\n", "", .) %>%
gsub("\t", "", .)
post_texts = c(post_texts, post)
}
}
extract_tesla = function(page_ori){
post_texts = c()
for(i in 1:10){
page = modify_url(page_ori, query=list(page=i))
board = read_html(page, encoding="UTF-8")
posts = board %>%
html_nodes('.threadloom-cardHeader') %>%
html_nodes('h1 > a')  %>%
html_attr('href') %>%
paste(addr1,.,sep = "")
posts
for(post_url in posts){
post = read_html(post_url) %>%
html_nodes('.messageContent') %>%
html_nodes('article > blockquote') %>%
html_text()  %>%
gsub("\n", "", .) %>%
gsub("\t", "", .)
post_texts = c(post_texts, post)
}
}
}
debug(extract_tesla)
extract_tesla(page_ori)
