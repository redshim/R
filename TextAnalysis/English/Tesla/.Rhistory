edgelist <- get.data.frame(g)
colnames(edgelist) <- c("source","target","value")
edgelist$source <- LETTERS[edgelist$source]
edgelist$target <- LETTERS[edgelist$target]
plot(
gvisSankey(edgelist, from="source",
to="target", weight="value",
options=list(
sankey="{link: {color: { fill: '#d799ae' } },
node: { width: 4,
color: { fill: '#a61d4c' },
label: { fontName: 'Times-Roman',
fontSize: 14,
color: '#871b47',
bold: true,
italic: true } }}"))
)
edgelist
g
E(g)$weight
g <- graph.tree(30, children = 4)
set.seed(123)
E(g)$weight = rpois(23, 4) + 1
g
get.data.frame(g)
edgelist <- get.data.frame(g)
colnames(edgelist) <- c("source","target","value")
edgelist$source <- LETTERS[edgelist$source]
edgelist$target <- LETTERS[edgelist$target]
edgelist
plot(
gvisSankey(edgelist, from="source",
to="target", weight="value",
options=list(
sankey="{link: {color: { fill: '#d799ae' } },
node: { width: 4,
color: { fill: '#a61d4c' },
label: { fontName: 'Times-Roman',
fontSize: 14,
color: '#871b47',
bold: true,
italic: true } }}"))
)
g <- graph.tree(100, children = 4)
set.seed(123)
E(g)$weight = rpois(23, 4) + 1
edgelist <- get.data.frame(g)
colnames(edgelist) <- c("source","target","value")
edgelist$source <- LETTERS[edgelist$source]
edgelist$target <- LETTERS[edgelist$target]
edgelist
plot(
gvisSankey(edgelist, from="source",
to="target", weight="value",
options=list(
sankey="{link: {color: { fill: '#d799ae' } },
node: { width: 4,
color: { fill: '#a61d4c' },
label: { fontName: 'Times-Roman',
fontSize: 14,
color: '#871b47',
bold: true,
italic: true } }}"))
)
d = read.csv2('C://Gephi//ext25.csv'
, header=F
, sep = ','
, quote = '"'
, stringsAsFactors = F)
UKvisits <- data.frame(origin=c(
"France", "Germany", "USA",
"Irish Republic", "Netherlands",
"Spain", "Italy", "Poland",
"Belgium", "Australia",
"Other countries", rep("UK", 5)),
visit=c(
rep("UK", 11), "Scotland",
"Wales", "Northern Ireland",
"England", "London"),
weights=c(
c(12,10,9,8,6,6,5,4,4,3,33)/100*31.8,
c(2.2,0.9,0.4,12.8,15.5)))
UKvisits
plot(
gvisSankey(df2, from="source",
to="target", weight="value",
options=list(
height=1000,
sankey="{link:{color:{fill:'lightblue'}}}"
))
)
require(googleVis)
plot(
gvisSankey(df, from="source",
to="target", weight="value",
options=list(
height=1000,
sankey="{link:{color:{fill:'lightblue'}}}"
))
)
df
df[1:5,]
df <- df[1:5,]
plot(
gvisSankey(df, from="source",
to="target", weight="value",
options=list(
height=1000,
sankey="{link:{color:{fill:'lightblue'}}}"
))
)
df <- df[1:4,]
plot(
gvisSankey(df, from="source",
to="target", weight="value",
options=list(
height=1000,
sankey="{link:{color:{fill:'lightblue'}}}"
))
)
plot(
gvisSankey(df, from="source",
to="target", weight="value",
options=list(
height=300,
sankey="{link:{color:{fill:'lightblue'}}}"
))
)
# load termDocMatrix
load("C:/R/igraph/termDocMatrix.rdata")
# inspect part of the matrix
termDocMatrix[5:10,1:20]
termDocMatrix[0:10,1:30]
as.matrix(termDocMatrix)
termDocMatrix <- as.matrix(termDocMatrix)
# change it to a Boolean matrix
termDocMatrix[termDocMatrix>=1] <- 1
termDocMatrix %*% t(termDocMatrix)
# transform into a term-term adjacency matrix
termMatrix <- termDocMatrix %*% t(termDocMatrix)
# inspect terms numbered 5 to 10
termMatrix[5:10,5:10]
library(igraph)
# build a graph from the above matrix
g <- graph.adjacency(termMatrix, weighted=T, mode = "undirected")
# remove loops
g <- simplify(g)
# set labels and degrees of vertices
V(g)$label <- V(g)$name
V(g)$degree <- degree(g)
######################################
# MBuild a Graph
######################################
# set seed to make the layout reproducible
set.seed(3952)
layout1 <- layout.fruchterman.reingold(g)
plot(g, layout=layout1)
######################################
# plot a Graph
######################################
plot(g, layout=layout.kamada.kawai)
tkplot(g, layout=layout.kamada.kawai)
######################################
# Make it Look Better
######################################
V(g)$label.cex <- 2.2 * V(g)$degree / max(V(g)$degree)+ .2
V(g)$label.color <- rgb(0, 0, .2, .8)
V(g)$frame.color <- NA
egam <- (log(E(g)$weight)+.4) / max(log(E(g)$weight)+.4)
E(g)$color <- rgb(.5, .5, 0, egam)
E(g)$width <- egam
# plot the graph in layout1
plot(g, layout=layout1)
termDocMatrix
termMatrix[5:10,5:10]
# build a graph from the above matrix
g <- graph.adjacency(termMatrix, weighted=T, mode = "undirected")
# remove loops
g <- simplify(g)
# set labels and degrees of vertices
V(g)$label <- V(g)$name
V(g)$degree <- degree(g)
# build a graph from the above matrix
g <- graph.adjacency(termMatrix, weighted=T, mode = "undirected")
# remove loops
g <- simplify(g)
# set labels and degrees of vertices
V(g)$label <- V(g)$name
V(g)$degree <- degree(g)
######################################
# MBuild a Graph
######################################
# set seed to make the layout reproducible
set.seed(3952)
layout1 <- layout.fruchterman.reingold(g)
plot(g, layout=layout1)
# plot a Graph
######################################
plot(g, layout=layout.kamada.kawai)
tkplot(g, layout=layout.kamada.kawai)
plot(g, layout=layout.kamada.kawai)
V(g)$label.cex
V(g)$label.cex <- 2.2 * V(g)$degree / max(V(g)$degree)+ .2
V(g)$label.color <- rgb(0, 0, .2, .8)
V(g)$frame.color <- NA
egam <- (log(E(g)$weight)+.4) / max(log(E(g)$weight)+.4)
E(g)$color <- rgb(.5, .5, 0, egam)
E(g)$width <- egam
# plot the graph in layout1
plot(g, layout=layout1)
myTdm.mat
## Delete All Old variables in R Session
rm(list=ls())
gc() # Garbage Collection = Clear memory Usage
options(mc.cores=1) # Use Core 1
# Working Directory Setting
setwd("C://R//TextAnalysis//English//Tesla")
getwd()
# textmining package install
#install.packages(c("data.table","tm","wordcloud","ggplot2","qgraph","slam","topicmodels","lad","LDAvis","servr","readxl"))
#install.packages(c("wordcloud2","SnowballC"))
library(data.table)
library(tm)
library(wordcloud2)
library(ggplot2)
library(qgraph)
library(SnowballC)
library(readxl)
library(tokenizers)
# Remove Process 가비지 데이터 정제
removeDoc <- function(x) {gsub("@[[:graph:]]*", "", x)}
removeURL <- function(x) { gsub("http://[[:graph:]]*", "", x)}
trim <- function (x) {gsub("^\\s+|\\s+$", "", x)}
removenum <- function (x) {gsub("\\d+", "", x)}
df$ptext <- tolower(df$CONTENTS)
df$ptext <- sapply(df$ptext, removeURL)
df$ptext <- sapply(df$ptext, removeDoc)
df$ptext <- sapply(df$ptext, trim)
df$ptext <- sapply(df$ptext, removenum)
df$ptext <- sapply(df$ptext, function(x) {gsub("if", "", x)})
df$ptext[1]
stopwords('en')
myStopwords <- c(stopwords('en'),"rt","the","it","is","this","would","the",
"i","one","of","or","be","and","did","now","going","has","could","aa","aaa",
"aaaa","aan","aand","aantal","aaronps","aaronua","ab","when", "like","all","have","had","just","can","abcd"
,"what","some","other","said","its","which","because","than","you","about","how","were"
,"ive","been","them","your","from","car")
tokenize_words(df$ptext[1], stopwords = myStopwords)
en_tokenizer = function(x){
x <- as.character(x)
x_token = tokenize_words(x, stopwords = myStopwords)
x_token = as.data.frame(x_token)
return(x_token[,1])
}
en_tokenizer(df$ptext[1])
# Create Corpus Object
#
##################################################################################
library(tm)
# 1.keyword analysis
# tdm (Term Document matrix)
#build corpus 코퍼스 생성
cps <- Corpus(VectorSource(df$ptext))
#summary(cps)
inspect(cps[1000])
cps <- tm_map(cps, removePunctuation)
cps <- tm_map(cps, removeNumbers)
#cps <- tm_map(cps, stemDocument)
cps <- tm_map(cps, stripWhitespace)
cps <- tm_map(cps, stemDocument, language = "english")
#remove potentially problematic symbols
#필요없는 문제 제거
toSpace <- content_transformer(function(x, pattern) { return (gsub(pattern, " ", x))})
cps <- tm_map(cps, toSpace, "-")
library(tm)
# 1.keyword analysis
# tdm (Term Document matrix)
cps <- Corpus(VectorSource(df$ptext))
df$ptext <- tolower(df$CONTENTS)
df$ptext <- sapply(df$ptext, removeURL)
df$ptext <- sapply(df$ptext, removeDoc)
df$ptext <- sapply(df$ptext, trim)
df$ptext <- sapply(df$ptext, removenum)
df$ptext <- sapply(df$ptext, function(x) {gsub("if", "", x)})
df$ptext[1]
stopwords('en')
myStopwords <- c(stopwords('en'),"rt","the","it","is","this","would","the",
"i","one","of","or","be","and","did","now","going","has","could","aa","aaa",
"aaaa","aan","aand","aantal","aaronps","aaronua","ab","when", "like","all","have","had","just","can","abcd"
,"what","some","other","said","its","which","because","than","you","about","how","were"
,"ive","been","them","your","from","car")
tokenize_words(df$ptext[1], stopwords = myStopwords)
en_tokenizer = function(x){
x <- as.character(x)
x_token = tokenize_words(x, stopwords = myStopwords)
x_token = as.data.frame(x_token)
return(x_token[,1])
}
en_tokenizer(df$ptext[1])
# import the data from the Source Files
d <- read_excel("3.noise.xlsx")
writeLines(as.character(d$CONTENTS[[1]]))
df <- as.data.frame(d)
getwd()
d <- read_excel("3.noise.xlsx")
# import the data from the Source Files
d <- read_excel("3.noise.xlsx")
writeLines(as.character(d$CONTENTS[[1]]))
df <- as.data.frame(d)
removeDoc <- function(x) {gsub("@[[:graph:]]*", "", x)}
removeURL <- function(x) { gsub("http://[[:graph:]]*", "", x)}
trim <- function (x) {gsub("^\\s+|\\s+$", "", x)}
removenum <- function (x) {gsub("\\d+", "", x)}
df$ptext <- tolower(df$CONTENTS)
df$ptext <- sapply(df$ptext, removeURL)
df$ptext <- sapply(df$ptext, removeDoc)
df$ptext <- sapply(df$ptext, trim)
df$ptext <- sapply(df$ptext, removenum)
df$ptext <- sapply(df$ptext, function(x) {gsub("if", "", x)})
df$ptext[1]
myStopwords <- c(stopwords('en'),"rt","the","it","is","this","would","the",
"i","one","of","or","be","and","did","now","going","has","could","aa","aaa",
"aaaa","aan","aand","aantal","aaronps","aaronua","ab","when", "like","all","have","had","just","can","abcd"
,"what","some","other","said","its","which","because","than","you","about","how","were"
,"ive","been","them","your","from","car")
tokenize_words(df$ptext[1], stopwords = myStopwords)
en_tokenizer = function(x){
x <- as.character(x)
x_token = tokenize_words(x, stopwords = myStopwords)
x_token = as.data.frame(x_token)
return(x_token[,1])
}
en_tokenizer(df$ptext[1])
library(tm)
# 1.keyword analysis
# tdm (Term Document matrix)
#build corpus 코퍼스 생성
cps <- Corpus(VectorSource(df$ptext))
#summary(cps)
inspect(cps[1000])
cps <- tm_map(cps, removePunctuation)
cps <- tm_map(cps, removeNumbers)
#cps <- tm_map(cps, stemDocument)
cps <- tm_map(cps, stripWhitespace)
cps <- tm_map(cps, stemDocument, language = "english")
#remove potentially problematic symbols
#필요없는 문제 제거
toSpace <- content_transformer(function(x, pattern) { return (gsub(pattern, " ", x))})
cps <- tm_map(cps, toSpace, "-")
tdm <- TermDocumentMatrix(cps,
control=list(tokenize = en_tokenizer
, wordLengths=c(3,8))) # extract over 2 word
inspect(tdm)
#Document Term Matrix with TF-IDF Score
dtm <- DocumentTermMatrix(cps,
control = list(weighting = function(x) weightTfIdf(x, normalize = FALSE),
tokenize = en_tokenizer,
stopwords = TRUE,
removePunctuation=T, # Remove Punctuation
removeNumbers=T, # Remove Number
wordLengths=c(3,8)
))
tdm_tf <- t(dtm)
library(wordcloud)
m <- as.matrix(tdm_tf)
wordFreq <- sort(rowSums(m),decreasing=TRUE)
head(wordFreq)
wordFreq_F <- wordFreq
wordFreq_F <- subset(wordFreq, wordFreq > 310)
#wordFreq_F
#wordFreq_F <- subset(wordFreq_F, wordFreq_F < 500)
length(wordFreq_F)
#wordFreq_F
set.seed(length(wordFreq_F))
pal <- brewer.pal(8,"Dark2")
windowsFonts(song=windowsFont("MS Song"))
wordcloud(words=names(wordFreq_F),freq=wordFreq_F
,min.freq= 4,random.order=F ,rot.per=.1 ,colors=pal ,family="malgun")
word.count = as.array(rollup(tdm_tf, 2)) # Rolling up group by the term
# frequently used words
word.order = order(word.count, decreasing = T)
# extract most top 50 word
most50 = word.order[1:50]
myTdm.mat = as.matrix(tdm_tf[most50,])
myTdm.mat
tdm_tf
word.count = as.array(rollup(tdm_tf, 2)) # Rolling up group by the term
library(slam)
word.count = as.array(rollup(tdm_tf, 2)) # Rolling up group by the term
# frequently used words
word.order = order(word.count, decreasing = T)
# extract most top 50 word
most50 = word.order[1:50]
myTdm.mat = as.matrix(tdm_tf[most50,])
myTdm.mat
myTdm.mat
t(myTdm.mat)
# Correlation Matrix
cormat = cor(t(myTdm.mat))
attributes(cormat)
for (i in seq(1,50)){
for(j in seq(1,50)){
if (cormat[i,j] <= 0){
cormat[i,j] = 0
}
}
}
graph <- graph.adjacency(cormat>0.5, weighted=TRUE, mode="upper")
E(graph)$weight<-t(cormat)[abs(t(cor_mat))>0.5]
E(graph)[ weight>0.7 ]$color <- "black"
E(graph)[ weight>=0.65 & weight<0.7 ]$color <- "red"
E(graph)[ weight>=0.6 &weight<0.65 ]$color <- "green"
E(graph)[ weight>=0.55 &weight<0.6 ]$color <- "blue"
E(graph)[ weight<0.55  ]$color <- "yellow"
V(graph)$label<- row.names(cormat)#V(graph)$name
graph$layout <- layout.fruchterman.reingold
plot(graph)
library(qgraph)
library(igraph)
graph <- graph.adjacency(cormat>0.5, weighted=TRUE, mode="upper")
E(graph)$weight<-t(cormat)[abs(t(cor_mat))>0.5]
E(graph)[ weight>0.7 ]$color <- "black"
E(graph)[ weight>=0.65 & weight<0.7 ]$color <- "red"
E(graph)[ weight>=0.6 &weight<0.65 ]$color <- "green"
E(graph)[ weight>=0.55 &weight<0.6 ]$color <- "blue"
E(graph)[ weight<0.55  ]$color <- "yellow"
V(graph)$label<- row.names(cormat)#V(graph)$name
graph$layout <- layout.fruchterman.reingold
plot(graph)
graph <- graph.adjacency(cormat>0.5, weighted=TRUE, mode="upper")
E(graph)$weight<-t(cormat)[abs(t(cor_mat))>0.5]
E(graph)[ weight>0.7 ]$color <- "black"
E(graph)[ weight>=0.65 & weight<0.7 ]$color <- "red"
E(graph)[ weight>=0.6 &weight<0.65 ]$color <- "green"
E(graph)[ weight>=0.55 &weight<0.6 ]$color <- "blue"
E(graph)[ weight<0.55  ]$color <- "yellow"
V(graph)$label<- row.names(cormat)#V(graph)$name
graph$layout <- layout.fruchterman.reingold
graph <- graph.adjacency(cormat>0.5, weighted=TRUE, mode="upper")
E(graph)$weight<-t(cormat)[abs(t(cormat))>0.5]
E(graph)[ weight>0.7 ]$color <- "black"
E(graph)[ weight>=0.65 & weight<0.7 ]$color <- "red"
E(graph)[ weight>=0.6 &weight<0.65 ]$color <- "green"
E(graph)[ weight>=0.55 &weight<0.6 ]$color <- "blue"
E(graph)[ weight<0.55  ]$color <- "yellow"
V(graph)$label<- row.names(cormat)#V(graph)$name
graph$layout <- layout.fruchterman.reingold
plot(graph)
for (i in seq(1,50)){
for(j in seq(1,50)){
if (cormat[i,j] <= 0){
cormat[i,j] = 0
}
}
}
library(qgraph)
library(igraph)
graph <- graph.adjacency(cormat>0.5, weighted=TRUE, mode="upper")
E(graph)$weight<-t(cormat)[abs(t(cormat))>0.5]
E(graph)[ weight>0.7 ]$color <- "black"
E(graph)[ weight>=0.65 & weight<0.7 ]$color <- "red"
E(graph)[ weight>=0.6 &weight<0.65 ]$color <- "green"
E(graph)[ weight>=0.55 &weight<0.6 ]$color <- "blue"
E(graph)[ weight<0.55  ]$color <- "yellow"
V(graph)$label<- row.names(cormat)#V(graph)$name
graph$layout <- layout.fruchterman.reingold
plot(graph)
# Correlation Matrix
cormat = cor(t(myTdm.mat))
#cormat = cormat[,1:2]
dfcormat = as.data.frame(myTdm.mat)
# Visualization
library(qgraph)
library(igraph)
library(showtext)
#pdf("Co-Occurence1_tf_idf.pdf")
#showtext.begin()
qgraph(cormat, labels=rownames(cormat)
, diag=F
, layout='spring'
, theme = 'Borkulo'
, label.scale=F
, label.fill.horizontal = 1
, legend.cex = .7
)
title("Co-occourance words between the sentence", line = 2.5)
# Correlation Matrix
cormat = cor(t(myTdm.mat))
attributes(cormat)
for (i in seq(1,50)){
for(j in seq(1,50)){
if (cormat[i,j] <= 0){
cormat[i,j] = 0
}
}
}
# Visualization
library(qgraph)
library(igraph)
library(showtext)
#pdf("Co-Occurence1_tf_idf.pdf")
#showtext.begin()
qgraph(cormat, labels=rownames(cormat)
, diag=F
, layout='spring'
, theme = 'Borkulo'
, label.scale=F
, label.fill.horizontal = 1
, legend.cex = .7
)
title("Co-occourance words between the sentence", line = 2.5)
#showtext.en
